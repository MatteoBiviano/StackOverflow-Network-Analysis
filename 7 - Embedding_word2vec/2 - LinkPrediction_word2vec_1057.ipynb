{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c1ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5014267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import optimizers, utils, initializers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c555a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import linkpred\n",
    "from linkpred.evaluation import Pair\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25c2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d61eaaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "474bb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('universe_1057.pickle', 'rb') as f:\n",
    "    universe_1057 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e0e5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pickle_1057 = pd.read_pickle('embedding_pickle_1057.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37b1cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_1057 = pd.read_pickle('edges_original_1057.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33d33b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unconnected_pairs_1057 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02906d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_isequal(x, a, b):\n",
    "    if x[0] == a:\n",
    "        if x[1] == b:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ui in universe_1057:\n",
    "    is_not = False\n",
    "    for i,j in edges_1057.values:\n",
    "        if check_isequal(ui, i, j):\n",
    "            is_not = True\n",
    "            break\n",
    "    if not is_not:\n",
    "        all_unconnected_pairs_1057.append((ui[0], ui[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd2ed61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_1057 = Word2Vec.load(\"word2vec_1057.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584465b",
   "metadata": {},
   "source": [
    "Collegamenti inesistenti => target negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f50394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_1_unlinked = [word2vec_1057.wv[i[0]] for i in all_unconnected_pairs_1057]\n",
    "node_2_unlinked = [word2vec_1057.wv[i[1]] for i in all_unconnected_pairs_1057]\n",
    "\n",
    "data_1057 = pd.DataFrame({'node_1':node_1_unlinked, \n",
    "                     'node_2':node_2_unlinked})\n",
    "\n",
    "# add target variable 'link'\n",
    "data_1057['link'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a00f1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.0069656703, 0.029779186, 0.009017678, 0.00...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.0126319, 0.016336868, 0.0054140245, 0.0140...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.0008716128, -0.0060879146, 0.00083144417, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.028240306, 0.07025809, 0.01292928, 0.02887...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-8.130686e-05, 0.006160258, 7.864202e-06, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14116</th>\n",
       "      <td>[-0.011859793, 0.030063095, 0.0030605968, 0.01...</td>\n",
       "      <td>[-0.0035571323, 0.004586752, -0.005745997, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14117</th>\n",
       "      <td>[-0.0014969681, 0.0032398321, -0.008584774, -0...</td>\n",
       "      <td>[0.009714041, 0.0056256913, -0.0054485463, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14118</th>\n",
       "      <td>[0.008738875, 0.008218716, 0.01001266, -0.0075...</td>\n",
       "      <td>[-0.0063515347, 0.014933867, -0.0010938012, 0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14119</th>\n",
       "      <td>[-0.0043330723, 0.0023013626, -0.004587147, 0....</td>\n",
       "      <td>[-0.0011158087, -0.004170317, 0.009934974, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14120</th>\n",
       "      <td>[-0.0029818127, 0.00816642, 0.0014875735, 0.00...</td>\n",
       "      <td>[-0.0011158087, -0.004170317, 0.009934974, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14121 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  node_1  \\\n",
       "0      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "1      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "2      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "3      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "4      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "...                                                  ...   \n",
       "14116  [-0.011859793, 0.030063095, 0.0030605968, 0.01...   \n",
       "14117  [-0.0014969681, 0.0032398321, -0.008584774, -0...   \n",
       "14118  [0.008738875, 0.008218716, 0.01001266, -0.0075...   \n",
       "14119  [-0.0043330723, 0.0023013626, -0.004587147, 0....   \n",
       "14120  [-0.0029818127, 0.00816642, 0.0014875735, 0.00...   \n",
       "\n",
       "                                                  node_2  link  \n",
       "0      [-0.0069656703, 0.029779186, 0.009017678, 0.00...     0  \n",
       "1      [-0.0126319, 0.016336868, 0.0054140245, 0.0140...     0  \n",
       "2      [-0.0008716128, -0.0060879146, 0.00083144417, ...     0  \n",
       "3      [-0.028240306, 0.07025809, 0.01292928, 0.02887...     0  \n",
       "4      [-8.130686e-05, 0.006160258, 7.864202e-06, 0.0...     0  \n",
       "...                                                  ...   ...  \n",
       "14116  [-0.0035571323, 0.004586752, -0.005745997, -0....     0  \n",
       "14117  [0.009714041, 0.0056256913, -0.0054485463, -0....     0  \n",
       "14118  [-0.0063515347, 0.014933867, -0.0010938012, 0....     0  \n",
       "14119  [-0.0011158087, -0.004170317, 0.009934974, 0.0...     0  \n",
       "14120  [-0.0011158087, -0.004170317, 0.009934974, 0.0...     0  \n",
       "\n",
       "[14121 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0fdc78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_1057 = nx.read_graphml(\"H_1057.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "983ecfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 596\n",
      "Number of edges: 18011\n",
      "Average degree:  60.4396\n",
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 1055\n",
      "Number of edges: 28222\n",
      "Average degree:  53.5014\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(H_1057))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "760a2f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def omissibile_links(G, df_edges):\n",
    "    initial_node_count = len(G.nodes)\n",
    "\n",
    "    fb_df_temp = df_edges.copy()\n",
    "\n",
    "    # empty list to store removable links\n",
    "    omissible_links_index = []\n",
    "\n",
    "    for i in tqdm(df_edges.index.values):\n",
    "\n",
    "      # remove a node pair and build a new graph\n",
    "      G_temp = nx.from_pandas_edgelist(fb_df_temp.drop(index = i), \"first\", \"second\", create_using=nx.Graph())\n",
    "\n",
    "      # check there is no spliting of graph and number of nodes is same\n",
    "      if (nx.number_connected_components(G_temp) == 1) and (len(G_temp.nodes) == initial_node_count):\n",
    "        omissible_links_index.append(i)\n",
    "        fb_df_temp = fb_df_temp.drop(index = i)\n",
    "    return omissible_links_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "493151bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 28222/28222 [18:24<00:00, 25.55it/s]\n"
     ]
    }
   ],
   "source": [
    "omissible_links_1057 =  omissibile_links(H_1057, edges_1057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b79ca5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28222"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges_1057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "733b4eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27168"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(omissible_links_1057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6960ff0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555985"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(universe_1057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca71ecf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[0.0018646894, -0.005355322, -0.0040034023, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.047307365, 0.14367199, 0.025162756, 0.0548...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.0069656703, 0.029779186, 0.009017678, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.00590836, 0.03831571, 0.0015101158, 0.0112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.0482577, 0.11495901, 0.03649007, 0.0454520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28217</th>\n",
       "      <td>[-0.0014969681, 0.0032398321, -0.008584774, -0...</td>\n",
       "      <td>[0.0008920013, 0.0057646595, 0.0031202612, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28218</th>\n",
       "      <td>[0.008738875, 0.008218716, 0.01001266, -0.0075...</td>\n",
       "      <td>[-0.0063515347, 0.014933867, -0.0010938012, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28219</th>\n",
       "      <td>[-0.0043330723, 0.0023013626, -0.004587147, 0....</td>\n",
       "      <td>[-0.0011158087, -0.004170317, 0.009934974, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28220</th>\n",
       "      <td>[-0.010021437, -4.2288553e-05, 0.0023859974, -...</td>\n",
       "      <td>[0.008025321, -0.005744415, -0.006585103, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28221</th>\n",
       "      <td>[-0.0029818127, 0.00816642, 0.0014875735, 0.00...</td>\n",
       "      <td>[-0.0011158087, -0.004170317, 0.009934974, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28222 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   first  \\\n",
       "0      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "1      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "2      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "3      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "4      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "...                                                  ...   \n",
       "28217  [-0.0014969681, 0.0032398321, -0.008584774, -0...   \n",
       "28218  [0.008738875, 0.008218716, 0.01001266, -0.0075...   \n",
       "28219  [-0.0043330723, 0.0023013626, -0.004587147, 0....   \n",
       "28220  [-0.010021437, -4.2288553e-05, 0.0023859974, -...   \n",
       "28221  [-0.0029818127, 0.00816642, 0.0014875735, 0.00...   \n",
       "\n",
       "                                                  second  \n",
       "0      [0.0018646894, -0.005355322, -0.0040034023, 0....  \n",
       "1      [-0.047307365, 0.14367199, 0.025162756, 0.0548...  \n",
       "2      [-0.0069656703, 0.029779186, 0.009017678, 0.00...  \n",
       "3      [-0.00590836, 0.03831571, 0.0015101158, 0.0112...  \n",
       "4      [-0.0482577, 0.11495901, 0.03649007, 0.0454520...  \n",
       "...                                                  ...  \n",
       "28217  [0.0008920013, 0.0057646595, 0.0031202612, -0....  \n",
       "28218  [-0.0063515347, 0.014933867, -0.0010938012, 0....  \n",
       "28219  [-0.0011158087, -0.004170317, 0.009934974, 0.0...  \n",
       "28220  [0.008025321, -0.005744415, -0.006585103, -0.0...  \n",
       "28221  [-0.0011158087, -0.004170317, 0.009934974, 0.0...  \n",
       "\n",
       "[28222 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_pickle_1057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b818ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df_ghost_1057 = embedding_pickle_1057.loc[omissible_links_1057]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0037efce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[0.0018646894, -0.005355322, -0.0040034023, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.047307365, 0.14367199, 0.025162756, 0.0548...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.0069656703, 0.029779186, 0.009017678, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.00590836, 0.03831571, 0.0015101158, 0.0112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0010758704, -0.001456658, 0.0034318417, 0.0...</td>\n",
       "      <td>[-0.0482577, 0.11495901, 0.03649007, 0.0454520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28199</th>\n",
       "      <td>[0.0028836606, -0.0036937655, -0.0018668072, 0...</td>\n",
       "      <td>[-0.0020805988, -0.0077785985, 0.010502531, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28204</th>\n",
       "      <td>[-0.011859793, 0.030063095, 0.0030605968, 0.01...</td>\n",
       "      <td>[0.0026899849, 0.005205888, 0.00016618294, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28211</th>\n",
       "      <td>[0.00039434768, 0.015331181, -0.0025737472, 0....</td>\n",
       "      <td>[0.009714041, 0.0056256913, -0.0054485463, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28213</th>\n",
       "      <td>[0.0026899849, 0.005205888, 0.00016618294, 0.0...</td>\n",
       "      <td>[-0.0014969681, 0.0032398321, -0.008584774, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28214</th>\n",
       "      <td>[0.0026899849, 0.005205888, 0.00016618294, 0.0...</td>\n",
       "      <td>[0.0008920013, 0.0057646595, 0.0031202612, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27168 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   first  \\\n",
       "0      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "1      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "2      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "3      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "4      [0.0010758704, -0.001456658, 0.0034318417, 0.0...   \n",
       "...                                                  ...   \n",
       "28199  [0.0028836606, -0.0036937655, -0.0018668072, 0...   \n",
       "28204  [-0.011859793, 0.030063095, 0.0030605968, 0.01...   \n",
       "28211  [0.00039434768, 0.015331181, -0.0025737472, 0....   \n",
       "28213  [0.0026899849, 0.005205888, 0.00016618294, 0.0...   \n",
       "28214  [0.0026899849, 0.005205888, 0.00016618294, 0.0...   \n",
       "\n",
       "                                                  second  \n",
       "0      [0.0018646894, -0.005355322, -0.0040034023, 0....  \n",
       "1      [-0.047307365, 0.14367199, 0.025162756, 0.0548...  \n",
       "2      [-0.0069656703, 0.029779186, 0.009017678, 0.00...  \n",
       "3      [-0.00590836, 0.03831571, 0.0015101158, 0.0112...  \n",
       "4      [-0.0482577, 0.11495901, 0.03649007, 0.0454520...  \n",
       "...                                                  ...  \n",
       "28199  [-0.0020805988, -0.0077785985, 0.010502531, -0...  \n",
       "28204  [0.0026899849, 0.005205888, 0.00016618294, 0.0...  \n",
       "28211  [0.009714041, 0.0056256913, -0.0054485463, -0....  \n",
       "28213  [-0.0014969681, 0.0032398321, -0.008584774, -0...  \n",
       "28214  [0.0008920013, 0.0057646595, 0.0031202612, -0....  \n",
       "\n",
       "[27168 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_df_ghost_1057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dec706cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of removable edges\n",
    "fb_df_ghost_1057 = embedding_pickle_1057.loc[omissible_links_1057]\n",
    "\n",
    "fb_df_ghost_1057 = fb_df_ghost_1057.rename(columns={\"first\": \"node_1\", \"second\": \"node_2\"})\n",
    "# add the target variable 'link'\n",
    "fb_df_ghost_1057['link'] = 1\n",
    "\n",
    "data_1057 = data_1057.append(fb_df_ghost_1057[['node_1', 'node_2', 'link']], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90f8c663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    27168\n",
       "0    14121\n",
       "Name: link, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1057['link'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9e81e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1057.to_pickle(\"data_1057.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90122197",
   "metadata": {},
   "source": [
    "----------------- Lettura dati -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64633754",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1057 = pd.read_pickle('data_1057.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b656fce",
   "metadata": {},
   "source": [
    "# 2 - Classificatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95b21bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1057 = []\n",
    "for i in range(data_1057.shape[0]):\n",
    "    first = data_1057.iloc[i][0]\n",
    "    second = data_1057.iloc[i][1]\n",
    "    mixed = np.concatenate((first, second), 0)\n",
    "    X_1057.append(mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f4355ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_1057, xtest_1057, ytrain_1057, ytest_1057 = train_test_split(np.asarray(X_1057), data_1057['link'], \n",
    "                                                test_size = 0.3, \n",
    "                                                random_state = 35,\n",
    "                                                stratify=data_1057['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02c877",
   "metadata": {},
   "source": [
    "### 1 - DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2f8c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77d75902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_depth': [2, 5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10, 20],\n",
    "}\n",
    "scores = ['roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9d39b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def builtGridSearch( X_train, X_test, y_train, y_test, tuned_parameters, scores):\n",
    "    optimals = {}\n",
    "    for score in scores:\n",
    "        print(\"------- Score = \" + str(score) + \" ------- \\n\")\n",
    "        model = tree.DecisionTreeClassifier()\n",
    "        k_fold = StratifiedKFold(n_splits=5, random_state=42)\n",
    "        \n",
    "        print(\"> Fold = \" + str(k_fold) + \"\\n\")\n",
    "        clf = GridSearchCV(model, tuned_parameters, error_score='raise', cv=5, scoring = score, return_train_score=True)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"> Best Parameter set: \\n\")\n",
    "        best = clf.best_params_\n",
    "        print(best)\n",
    "        \n",
    "        print(\"\\n> Grid scores:\\n\")\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        \n",
    "        parameters = {\"criterion\": [], \"max_depth\": [], \"min_samples_leaf\": [], \"min_samples_split\": []}\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "            parameters[\"criterion\"].append(params[\"criterion\"])\n",
    "\n",
    "            if(params[\"max_depth\"] == None):\n",
    "                parameters[\"max_depth\"].append(0)\n",
    "            else:\n",
    "                parameters[\"max_depth\"].append(params[\"max_depth\"])\n",
    "\n",
    "            #parameters[\"max_depth\"].append(params[\"max_depth\"])\n",
    "            parameters[\"min_samples_leaf\"].append(params[\"min_samples_leaf\"])\n",
    "            parameters[\"min_samples_split\"].append(params[\"min_samples_split\"])\n",
    "        \n",
    "        print(\"-> Report\\n\") \n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print(\"\\n\")\n",
    "        print(\"**** Matrice di Confusione *****\")\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        print(' True negative: %d False negative: %d' % (tn, fn))\n",
    "        print(' True positive: %d False positive: %d' % (tp, fp))\n",
    "\n",
    "        \n",
    "        # Creazione del csv\n",
    "        d = {score: means, 'std_dev': stds*2, 'criterion': parameters[\"criterion\"], \"max_depth\": parameters[\"max_depth\"], \"min_samples_leaf\": parameters[\"min_samples_leaf\"], \"min_samples_split\": parameters[\"min_samples_split\"]}\n",
    "        \n",
    "        dataF = pd.DataFrame(data=d)\n",
    "        \n",
    "        dataF.to_csv(\"result_DT_gridsearch.csv\",index=False, header=True)\n",
    "        \n",
    "\n",
    "        print(\"...........RESULTS FOR TRAINING.........\")\n",
    "        print(\"........................................\")\n",
    "        means = clf.cv_results_['mean_train_score']\n",
    "        stds = clf.cv_results_['std_train_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))        \n",
    "        \n",
    "        print(\"____________________________________________\")\n",
    "        \n",
    "        optimals[score] = best\n",
    "    return optimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29c1f31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Score = roc_auc ------- \n",
      "\n",
      "> Fold = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Best Parameter set: \n",
      "\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "\n",
      "> Grid scores:\n",
      "\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.527 (+/-0.008) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.559 (+/-0.030) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.559 (+/-0.030) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.559 (+/-0.030) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.559 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.559 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.559 (+/-0.030) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.559 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.560 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.559 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.559 (+/-0.030) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.559 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.559 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.560 (+/-0.030) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.560 (+/-0.030) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.560 (+/-0.030) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.560 (+/-0.030) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.551 (+/-0.021) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.552 (+/-0.021) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.552 (+/-0.021) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.554 (+/-0.023) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.551 (+/-0.024) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.551 (+/-0.025) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.551 (+/-0.024) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.553 (+/-0.025) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.555 (+/-0.028) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.555 (+/-0.028) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.555 (+/-0.028) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.555 (+/-0.028) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.562 (+/-0.025) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.562 (+/-0.025) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.562 (+/-0.025) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.562 (+/-0.025) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.453 (+/-0.034) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.455 (+/-0.034) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.460 (+/-0.031) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.475 (+/-0.030) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.468 (+/-0.032) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.468 (+/-0.031) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.469 (+/-0.032) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.479 (+/-0.030) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.484 (+/-0.034) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.483 (+/-0.031) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.483 (+/-0.034) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.483 (+/-0.031) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.500 (+/-0.020) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.501 (+/-0.022) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.501 (+/-0.020) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.500 (+/-0.020) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.343 (+/-0.004) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.353 (+/-0.005) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.367 (+/-0.007) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.400 (+/-0.012) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.391 (+/-0.010) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.391 (+/-0.013) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.392 (+/-0.012) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.413 (+/-0.015) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.427 (+/-0.019) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.426 (+/-0.017) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.427 (+/-0.018) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.428 (+/-0.019) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.473 (+/-0.017) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.474 (+/-0.018) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.473 (+/-0.019) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.473 (+/-0.018) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.527 (+/-0.008) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.556 (+/-0.021) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.556 (+/-0.021) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.556 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.568 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.569 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.569 (+/-0.012) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.570 (+/-0.015) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.569 (+/-0.013) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.570 (+/-0.013) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.570 (+/-0.013) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.570 (+/-0.013) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.567 (+/-0.020) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.568 (+/-0.020) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.567 (+/-0.019) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.567 (+/-0.019) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.570 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.570 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.570 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.570 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.507 (+/-0.027) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.508 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.510 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.514 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.509 (+/-0.023) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.509 (+/-0.023) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.509 (+/-0.024) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.513 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.506 (+/-0.027) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.505 (+/-0.028) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.506 (+/-0.026) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.505 (+/-0.028) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.517 (+/-0.016) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.516 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.517 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.516 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.346 (+/-0.005) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.353 (+/-0.006) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.365 (+/-0.008) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.388 (+/-0.012) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.395 (+/-0.015) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.393 (+/-0.016) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.392 (+/-0.020) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.414 (+/-0.017) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.427 (+/-0.019) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.429 (+/-0.017) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.429 (+/-0.020) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.427 (+/-0.019) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.473 (+/-0.009) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.474 (+/-0.010) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.473 (+/-0.008) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.473 (+/-0.009) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "-> Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.02      0.04      4236\n",
      "           1       0.66      0.97      0.78      8151\n",
      "\n",
      "    accuracy                           0.64     12387\n",
      "   macro avg       0.46      0.50      0.41     12387\n",
      "weighted avg       0.52      0.64      0.53     12387\n",
      "\n",
      "\n",
      "\n",
      "**** Matrice di Confusione *****\n",
      " True negative: 94 False negative: 261\n",
      " True positive: 7890 False positive: 4142\n",
      "...........RESULTS FOR TRAINING.........\n",
      "........................................\n",
      "0.534 (+/-0.007) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.534 (+/-0.007) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.534 (+/-0.007) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.534 (+/-0.007) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.534 (+/-0.006) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.585 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.584 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.584 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.584 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.584 (+/-0.014) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.693 (+/-0.024) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.693 (+/-0.024) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.692 (+/-0.024) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.689 (+/-0.024) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.690 (+/-0.023) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.690 (+/-0.023) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.690 (+/-0.023) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.688 (+/-0.023) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.687 (+/-0.024) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.687 (+/-0.024) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.687 (+/-0.024) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.687 (+/-0.024) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.681 (+/-0.022) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.681 (+/-0.023) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.681 (+/-0.022) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.681 (+/-0.022) for {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.816 (+/-0.030) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.814 (+/-0.029) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.809 (+/-0.027) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.796 (+/-0.024) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.803 (+/-0.026) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.803 (+/-0.026) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.803 (+/-0.026) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.791 (+/-0.023) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.786 (+/-0.021) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.786 (+/-0.021) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.786 (+/-0.021) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.786 (+/-0.021) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.761 (+/-0.013) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.761 (+/-0.013) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.761 (+/-0.013) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.761 (+/-0.013) for {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.924 (+/-0.001) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.921 (+/-0.001) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.912 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.886 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.893 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.893 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.893 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.870 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.852 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.852 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.852 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.852 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.797 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.797 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.797 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.797 (+/-0.002) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.534 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.582 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.581 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.581 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.581 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.581 (+/-0.014) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.667 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.666 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.666 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.664 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.665 (+/-0.017) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.665 (+/-0.016) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.665 (+/-0.016) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.664 (+/-0.016) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.665 (+/-0.018) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.665 (+/-0.018) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.665 (+/-0.018) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.665 (+/-0.018) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.663 (+/-0.018) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.663 (+/-0.018) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.663 (+/-0.018) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.663 (+/-0.018) for {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.762 (+/-0.010) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.760 (+/-0.011) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.758 (+/-0.010) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.752 (+/-0.009) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.760 (+/-0.011) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.760 (+/-0.011) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.760 (+/-0.011) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.753 (+/-0.009) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.760 (+/-0.010) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.760 (+/-0.010) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.760 (+/-0.010) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.760 (+/-0.010) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.745 (+/-0.007) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.745 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.744 (+/-0.007) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.745 (+/-0.006) for {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "0.924 (+/-0.001) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.922 (+/-0.001) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.916 (+/-0.001) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "0.897 (+/-0.001) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 20}\n",
      "0.898 (+/-0.002) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "0.899 (+/-0.003) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "0.899 (+/-0.003) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "0.877 (+/-0.002) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 20}\n",
      "0.857 (+/-0.003) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "0.857 (+/-0.002) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "0.857 (+/-0.002) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "0.857 (+/-0.003) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "0.799 (+/-0.004) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 2}\n",
      "0.799 (+/-0.004) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 5}\n",
      "0.799 (+/-0.004) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
      "0.799 (+/-0.004) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 20}\n",
      "____________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimals_1057 = builtGridSearch(xtrain_1057, xtest_1057, ytrain_1057, ytest_1057, tuned_parameters, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b5d47aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc': {'criterion': 'entropy',\n",
       "  'max_depth': 10,\n",
       "  'min_samples_leaf': 1,\n",
       "  'min_samples_split': 20}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimals_1057"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e00d5",
   "metadata": {},
   "source": [
    "# 1 - DecisionTree_Bilanciato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a2849",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_n = data_1057['link'].value_counts()[0] - data_1057['link'].value_counts()[1]\n",
    "remove_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1899a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = data_1057.copy()\n",
    "drop_indices = np.random.choice(df[df[\"link\"]==0].index, remove_n, replace=False)\n",
    "df_subset = df.drop(drop_indices)\n",
    "print(df_subset['link'].value_counts())\n",
    "\n",
    "X_sbset_1057 = []\n",
    "for i in range(df_subset.shape[0]):\n",
    "    first = df_subset.iloc[i][0]\n",
    "    second = df_subset.iloc[i][1]\n",
    "    mixed = np.concatenate((first, second), 0)\n",
    "    X_sbset_597.append(mixed)\n",
    "    \n",
    "    \n",
    "xtrain_sb_1057, xtest_sb_1057, ytrain_sb_1057, ytest_sb_1057 = train_test_split(np.asarray(X_sbset_1057), df_subset[\"link\"], \n",
    "                                                test_size = 0.3, \n",
    "                                                random_state = 35,\n",
    "                                                stratify=df_subset[\"link\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b494737",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimals_sb_1057 = builtGridSearch(xtrain_sb_1057, xtest_sb_1057, ytrain_sb_1057, ytest_sb_1057, tuned_parameters, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimals_sb_1057"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f34fd2",
   "metadata": {},
   "source": [
    "### 3 - LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "446173f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d8d0d2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {\n",
    "    'tol':(1.0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6),\n",
    "    'C': (0.001, 0.05, 0.01, 0.1, 1.0, 10.0, 50, 100.0)\n",
    "}\n",
    "# Set the parameters by cross-validation\n",
    "k_fold = StratifiedKFold(n_splits=4, random_state=42)\n",
    "\n",
    "scores = ['roc_auc']\n",
    "\n",
    "def gridsearch_linearsvc(X_train, y_train, X_test, y_test, k_fold, tuned_parameters, scores):\n",
    "    optimals = {}\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for ----> %s\" % score)\n",
    "        print()\n",
    "\n",
    "        svm = LinearSVC()\n",
    "\n",
    "        clf = GridSearchCV(svm, tuned_parameters, error_score='raise', cv=k_fold, scoring=score)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print()\n",
    "        print(\"**** Matrice di Confusione *****\")\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        print(' True negative: %d False negative: %d' % (tn, fn))\n",
    "        print(' True positive: %d False positive: %d' % (tp, fp))\n",
    "        print(\"____________________________________________\")\n",
    "        optimals[score] = clf.best_params_\n",
    "    return optimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2acb6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for ----> roc_auc\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10.0, 'tol': 1e-05}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.519 (+/-0.007) for {'C': 0.001, 'tol': 1.0}\n",
      "0.518 (+/-0.007) for {'C': 0.001, 'tol': 0.1}\n",
      "0.518 (+/-0.007) for {'C': 0.001, 'tol': 0.01}\n",
      "0.518 (+/-0.007) for {'C': 0.001, 'tol': 0.001}\n",
      "0.518 (+/-0.007) for {'C': 0.001, 'tol': 0.0001}\n",
      "0.518 (+/-0.007) for {'C': 0.001, 'tol': 1e-05}\n",
      "0.518 (+/-0.007) for {'C': 0.001, 'tol': 1e-06}\n",
      "0.562 (+/-0.007) for {'C': 0.05, 'tol': 1.0}\n",
      "0.561 (+/-0.009) for {'C': 0.05, 'tol': 0.1}\n",
      "0.561 (+/-0.009) for {'C': 0.05, 'tol': 0.01}\n",
      "0.561 (+/-0.009) for {'C': 0.05, 'tol': 0.001}\n",
      "0.561 (+/-0.009) for {'C': 0.05, 'tol': 0.0001}\n",
      "0.561 (+/-0.009) for {'C': 0.05, 'tol': 1e-05}\n",
      "0.561 (+/-0.009) for {'C': 0.05, 'tol': 1e-06}\n",
      "0.538 (+/-0.014) for {'C': 0.01, 'tol': 1.0}\n",
      "0.537 (+/-0.010) for {'C': 0.01, 'tol': 0.1}\n",
      "0.537 (+/-0.010) for {'C': 0.01, 'tol': 0.01}\n",
      "0.537 (+/-0.010) for {'C': 0.01, 'tol': 0.001}\n",
      "0.537 (+/-0.010) for {'C': 0.01, 'tol': 0.0001}\n",
      "0.537 (+/-0.010) for {'C': 0.01, 'tol': 1e-05}\n",
      "0.537 (+/-0.010) for {'C': 0.01, 'tol': 1e-06}\n",
      "0.571 (+/-0.006) for {'C': 0.1, 'tol': 1.0}\n",
      "0.571 (+/-0.007) for {'C': 0.1, 'tol': 0.1}\n",
      "0.571 (+/-0.007) for {'C': 0.1, 'tol': 0.01}\n",
      "0.571 (+/-0.007) for {'C': 0.1, 'tol': 0.001}\n",
      "0.571 (+/-0.007) for {'C': 0.1, 'tol': 0.0001}\n",
      "0.571 (+/-0.007) for {'C': 0.1, 'tol': 1e-05}\n",
      "0.571 (+/-0.007) for {'C': 0.1, 'tol': 1e-06}\n",
      "0.584 (+/-0.008) for {'C': 1.0, 'tol': 1.0}\n",
      "0.585 (+/-0.006) for {'C': 1.0, 'tol': 0.1}\n",
      "0.585 (+/-0.006) for {'C': 1.0, 'tol': 0.01}\n",
      "0.585 (+/-0.006) for {'C': 1.0, 'tol': 0.001}\n",
      "0.585 (+/-0.006) for {'C': 1.0, 'tol': 0.0001}\n",
      "0.585 (+/-0.006) for {'C': 1.0, 'tol': 1e-05}\n",
      "0.585 (+/-0.006) for {'C': 1.0, 'tol': 1e-06}\n",
      "0.584 (+/-0.006) for {'C': 10.0, 'tol': 1.0}\n",
      "0.586 (+/-0.006) for {'C': 10.0, 'tol': 0.1}\n",
      "0.586 (+/-0.006) for {'C': 10.0, 'tol': 0.01}\n",
      "0.586 (+/-0.006) for {'C': 10.0, 'tol': 0.001}\n",
      "0.586 (+/-0.006) for {'C': 10.0, 'tol': 0.0001}\n",
      "0.586 (+/-0.006) for {'C': 10.0, 'tol': 1e-05}\n",
      "0.586 (+/-0.006) for {'C': 10.0, 'tol': 1e-06}\n",
      "0.584 (+/-0.006) for {'C': 50, 'tol': 1.0}\n",
      "0.586 (+/-0.005) for {'C': 50, 'tol': 0.1}\n",
      "0.586 (+/-0.006) for {'C': 50, 'tol': 0.01}\n",
      "0.586 (+/-0.006) for {'C': 50, 'tol': 0.001}\n",
      "0.586 (+/-0.006) for {'C': 50, 'tol': 0.0001}\n",
      "0.586 (+/-0.006) for {'C': 50, 'tol': 1e-05}\n",
      "0.586 (+/-0.006) for {'C': 50, 'tol': 1e-06}\n",
      "0.586 (+/-0.005) for {'C': 100.0, 'tol': 1.0}\n",
      "0.586 (+/-0.005) for {'C': 100.0, 'tol': 0.1}\n",
      "0.586 (+/-0.006) for {'C': 100.0, 'tol': 0.01}\n",
      "0.586 (+/-0.006) for {'C': 100.0, 'tol': 0.001}\n",
      "0.586 (+/-0.006) for {'C': 100.0, 'tol': 0.0001}\n",
      "0.585 (+/-0.005) for {'C': 100.0, 'tol': 1e-05}\n",
      "0.586 (+/-0.005) for {'C': 100.0, 'tol': 1e-06}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.03      0.06      4236\n",
      "           1       0.66      0.98      0.79      8151\n",
      "\n",
      "    accuracy                           0.65     12387\n",
      "   macro avg       0.55      0.51      0.43     12387\n",
      "weighted avg       0.58      0.65      0.54     12387\n",
      "\n",
      "\n",
      "**** Matrice di Confusione *****\n",
      " True negative: 145 False negative: 191\n",
      " True positive: 7960 False positive: 4091\n",
      "____________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "optimals_1057 = gridsearch_linearsvc(xtrain_1057,  ytrain_1057, xtest_1057, ytest_1057, k_fold, tuned_parameters, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimals_1057"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7171926",
   "metadata": {},
   "source": [
    "# 4 - NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e53e9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_1( X_train):\n",
    "    optimizer='adagrad'\n",
    "    #optimizer='adam'\n",
    "    n_feature = X_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=n_feature, activation='tanh'))\n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    # Linear\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"AUC\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2da72adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_2(X_train):\n",
    "    n_feature = X_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=n_feature, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    # Linear\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    sgd = optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=[\"AUC\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b24b2483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6439 - auc: 0.5058 - val_loss: 0.6425 - val_auc: 0.5116\n",
      "Epoch 2/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6422 - auc: 0.5103 - val_loss: 0.6425 - val_auc: 0.5126\n",
      "Epoch 3/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6422 - auc: 0.5125 - val_loss: 0.6424 - val_auc: 0.5123\n",
      "Epoch 4/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5135 - val_loss: 0.6424 - val_auc: 0.5124\n",
      "Epoch 5/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5138 - val_loss: 0.6424 - val_auc: 0.5112\n",
      "Epoch 6/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5152 - val_loss: 0.6424 - val_auc: 0.5140\n",
      "Epoch 7/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5153 - val_loss: 0.6424 - val_auc: 0.5131\n",
      "Epoch 8/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5176 - val_loss: 0.6425 - val_auc: 0.5119\n",
      "Epoch 9/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5163 - val_loss: 0.6424 - val_auc: 0.5117\n",
      "Epoch 10/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5161 - val_loss: 0.6424 - val_auc: 0.5136\n",
      "Epoch 11/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5166 - val_loss: 0.6425 - val_auc: 0.5128\n",
      "Epoch 12/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5175 - val_loss: 0.6424 - val_auc: 0.5131\n",
      "Epoch 13/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5151 - val_loss: 0.6424 - val_auc: 0.5139\n",
      "Epoch 14/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6421 - auc: 0.5170 - val_loss: 0.6424 - val_auc: 0.5132\n",
      "Epoch 15/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5173 - val_loss: 0.6424 - val_auc: 0.5136\n",
      "Epoch 16/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5180 - val_loss: 0.6424 - val_auc: 0.5144\n",
      "Epoch 17/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5177 - val_loss: 0.6424 - val_auc: 0.5137\n",
      "Epoch 18/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5177 - val_loss: 0.6424 - val_auc: 0.5146\n",
      "Epoch 19/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5194 - val_loss: 0.6424 - val_auc: 0.5142\n",
      "Epoch 20/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5198 - val_loss: 0.6424 - val_auc: 0.5150\n",
      "Epoch 21/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5192 - val_loss: 0.6424 - val_auc: 0.5156\n",
      "Epoch 22/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5194 - val_loss: 0.6424 - val_auc: 0.5139\n",
      "Epoch 23/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5203 - val_loss: 0.6423 - val_auc: 0.5143\n",
      "Epoch 24/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5203 - val_loss: 0.6423 - val_auc: 0.5140\n",
      "Epoch 25/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5181 - val_loss: 0.6423 - val_auc: 0.5134\n",
      "Epoch 26/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5212 - val_loss: 0.6423 - val_auc: 0.5156\n",
      "Epoch 27/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5213 - val_loss: 0.6423 - val_auc: 0.5163\n",
      "Epoch 28/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6419 - auc: 0.5214 - val_loss: 0.6424 - val_auc: 0.5145\n",
      "Epoch 29/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6420 - auc: 0.5205 - val_loss: 0.6423 - val_auc: 0.5149\n",
      "Epoch 30/30\n",
      "2891/2891 [==============================] - 4s 1ms/step - loss: 0.6419 - auc: 0.5209 - val_loss: 0.6423 - val_auc: 0.5146\n",
      "Epoch 1/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6445 - auc: 0.5038 - val_loss: 0.6425 - val_auc: 0.5000\n",
      "Epoch 2/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6436 - auc: 0.4972 - val_loss: 0.6433 - val_auc: 0.5000\n",
      "Epoch 3/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6436 - auc: 0.5024 - val_loss: 0.6424 - val_auc: 0.5000\n",
      "Epoch 4/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6432 - auc: 0.4964 - val_loss: 0.6424 - val_auc: 0.5000\n",
      "Epoch 5/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6433 - auc: 0.4996 - val_loss: 0.6486 - val_auc: 0.5000\n",
      "Epoch 6/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6437 - auc: 0.4941 - val_loss: 0.6423 - val_auc: 0.5000\n",
      "Epoch 7/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6435 - auc: 0.4964 - val_loss: 0.6427 - val_auc: 0.5000\n",
      "Epoch 8/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6430 - auc: 0.5050 - val_loss: 0.6437 - val_auc: 0.5044\n",
      "Epoch 9/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6433 - auc: 0.4991 - val_loss: 0.6433 - val_auc: 0.5032\n",
      "Epoch 10/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6436 - auc: 0.4943 - val_loss: 0.6451 - val_auc: 0.5025\n",
      "Epoch 11/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6432 - auc: 0.5045 - val_loss: 0.6523 - val_auc: 0.5117\n",
      "Epoch 12/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6439 - auc: 0.4993 - val_loss: 0.6424 - val_auc: 0.5037\n",
      "Epoch 13/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6432 - auc: 0.4975 - val_loss: 0.6430 - val_auc: 0.5118\n",
      "Epoch 14/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6437 - auc: 0.4972 - val_loss: 0.6428 - val_auc: 0.4976\n",
      "Epoch 15/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6432 - auc: 0.5004 - val_loss: 0.6440 - val_auc: 0.5000\n",
      "Epoch 16/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6435 - auc: 0.4968 - val_loss: 0.6424 - val_auc: 0.5000\n",
      "Epoch 17/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6432 - auc: 0.5000 - val_loss: 0.6427 - val_auc: 0.5000\n",
      "Epoch 18/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6433 - auc: 0.4974 - val_loss: 0.6471 - val_auc: 0.5000\n",
      "Epoch 19/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6433 - auc: 0.5012 - val_loss: 0.6428 - val_auc: 0.5154\n",
      "Epoch 20/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6434 - auc: 0.4956 - val_loss: 0.6507 - val_auc: 0.5029\n",
      "Epoch 21/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6433 - auc: 0.5010 - val_loss: 0.6423 - val_auc: 0.5027\n",
      "Epoch 22/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6433 - auc: 0.4992 - val_loss: 0.6428 - val_auc: 0.5077\n",
      "Epoch 23/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6434 - auc: 0.4986 - val_loss: 0.6429 - val_auc: 0.5121\n",
      "Epoch 24/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6435 - auc: 0.4990 - val_loss: 0.6428 - val_auc: 0.5014\n",
      "Epoch 25/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6433 - auc: 0.5001 - val_loss: 0.6431 - val_auc: 0.5119\n",
      "Epoch 26/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6427 - auc: 0.5146 - val_loss: 0.6434 - val_auc: 0.5114\n",
      "Epoch 27/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6433 - auc: 0.4978 - val_loss: 0.6453 - val_auc: 0.4997\n",
      "Epoch 28/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6436 - auc: 0.4971 - val_loss: 0.6424 - val_auc: 0.5125\n",
      "Epoch 29/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6432 - auc: 0.4996 - val_loss: 0.6503 - val_auc: 0.5002\n",
      "Epoch 30/30\n",
      "579/579 [==============================] - 1s 2ms/step - loss: 0.6436 - auc: 0.4972 - val_loss: 0.6431 - val_auc: 0.5070\n"
     ]
    }
   ],
   "source": [
    "model1_1057 = build_model_1(xtrain_1057)\n",
    "history1_1057 = model1_1057.fit(xtrain_1057, ytrain_1057, validation_data=(xtest_1057, ytest_1057), epochs=30, batch_size=10).history\n",
    "\n",
    "model2_1057 = build_model_2(xtrain_1057)\n",
    "history2_1057 = model2_1057.fit(xtrain_1057, ytrain_1057, validation_data=(xtest_1057, ytest_1057), epochs=30, batch_size=50).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b0db87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388/388 [==============================] - 0s 918us/step - loss: 0.6423 - auc: 0.5146\n",
      "388/388 [==============================] - 0s 911us/step - loss: 0.6431 - auc: 0.5070\n",
      "Loss 0.642318, AUC 0.514630\n",
      "Loss 0.643136, AUC 0.506970\n"
     ]
    }
   ],
   "source": [
    "test_loss_1_1057, test_auc_1_1057 = model1_1057.evaluate(xtest_1057, ytest_1057)\n",
    "test_loss_2_1057, test_auc_2_1057 = model2_1057.evaluate(xtest_1057, ytest_1057)\n",
    "\n",
    "print('Loss %f, AUC %f' % (test_loss_1_1057, test_auc_1_1057))\n",
    "print('Loss %f, AUC %f' % (test_loss_2_1057, test_auc_2_1057))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7afa2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe67a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b160f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X_train, learning_rate=0.1, momentum = 0.9, nesterov = True,\n",
    "                 activation = 'sigmoid', regularizer = 0.01, units=3, hidden_layers=1):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    #initializer = initializers.RandomUniform(minval=0, maxval=0.5, seed=None)\n",
    "    model.add(Dense(units, input_dim=X_train.shape[1], activation = activation, \n",
    "                         kernel_regularizer=regularizers.l2(regularizer)))  \n",
    "    for i in range(1,hidden_layers):\n",
    "          model.add(Dense(units, activation = activation, \n",
    "                         kernel_regularizer=regularizers.l2(regularizer)))  \n",
    "    model.add(Dense(1, activation = 'sigmoid',  kernel_regularizer=regularizers.l2(regularizer)))\n",
    "    sgd = optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=nesterov)\n",
    "    model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7d511a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = {'batch_size': [128],\n",
    "              'epochs': [100],\n",
    "             'learning_rate': [0.261],\n",
    "             'momentum':[0.9],\n",
    "             'nesterov':[False],\n",
    "             'activation':['tanh'],\n",
    "              'regularizer':[0.0001],\n",
    "              'units': [10],\n",
    "             'hidden_layers': [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba1142cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, loss = 0, accuracy = 0, batch_size = 0, epochs = 0, learning_rate=0, momentum=0,nesterov=False,\n",
    "                activation='sigmoid', regularizer=0, units=0):\n",
    "        self.accuracy_CV_list = []\n",
    "        self.accuracy = accuracy\n",
    "        self.loss_CV_list = []\n",
    "        self.loss = loss\n",
    "        self.mee_list = []\n",
    "        self.mee = 0\n",
    "        self.std = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.activation = activation\n",
    "        self.regularizer = regularizer\n",
    "        self.units = units\n",
    "        self.noise = 0\n",
    "    def toString(self):\n",
    "        print(\"\"\"{MEE: %f (+/- %0.2f), Number of Units: %d, Batch Size: %d, Epochs: %d, Learning Rate: %f, Momentum: %f,\n",
    "              \"Nesterov: %s, Activation: %s, Regularization: %f, Noise: %f}\"\"\" % \n",
    "              (self.mee, self.std, self.units, self.batch_size, self.epochs, self.learning_rate, self.momentum,\n",
    "              self.nesterov, self.activation, self.regularizer, self.noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c1d3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(X_train, Y_train, parameters, cv = 3, rand=False):\n",
    "    models = []\n",
    "    allNames = list(parameters)\n",
    "    combinations = itertools.product(*(parameters[Name] for Name in allNames))\n",
    "    searchList = []\n",
    "    if rand:\n",
    "        searchList = random.sample(list(combinations), k=100)\n",
    "    else:\n",
    "        searchList = list(combinations)\n",
    "    for i in searchList:\n",
    "        print(searchList.index(i), i)\n",
    "        batch_size = i[0]\n",
    "        epochs=i[1]\n",
    "        learning_rate=i[2]\n",
    "        momentum=i[3]\n",
    "        nesterov=i[4]\n",
    "        activation=i[5]\n",
    "        regularizer=i[6]\n",
    "        units = i[7]\n",
    "        temp = Model(batch_size=batch_size, epochs=epochs, learning_rate=learning_rate,\n",
    "                     momentum=momentum, nesterov=nesterov, activation=activation, regularizer=regularizer, units=units)\n",
    "        model = None # Clearing the NN.\n",
    "        model = create_model(X_train, learning_rate=learning_rate, momentum=momentum, nesterov=nesterov, \n",
    "                                 activation=activation, regularizer=regularizer, units=units)\n",
    "        r = model.fit(X_train, Y_train,validation_split = float(1.0/cv),\n",
    "                      batch_size =batch_size, epochs=epochs, verbose=1)\n",
    "        loss = r.history['val_loss'][-1]\n",
    "        #accuracy = r.history['val_acc'][-1]\n",
    "        temp.loss = loss\n",
    "        #temp.accuracy = accuracy\n",
    "        models.append(temp) \n",
    "          \n",
    "    result = sorted(models, key=lambda x: x.loss, reverse=False)\n",
    "    return result, r, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cd7ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (128, 100, 0.261, 0.9, False, 'tanh', 0.0001, 10, 1)\n",
      "Epoch 1/100\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 0.6474 - auc: 0.4973 - val_loss: 0.6478 - val_auc: 0.5115\n",
      "Epoch 2/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6445 - auc: 0.4965 - val_loss: 0.6485 - val_auc: 0.5146\n",
      "Epoch 3/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6435 - auc: 0.4969 - val_loss: 0.6464 - val_auc: 0.5188\n",
      "Epoch 4/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6435 - auc: 0.5015 - val_loss: 0.6530 - val_auc: 0.5183\n",
      "Epoch 5/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6433 - auc: 0.5026 - val_loss: 0.6458 - val_auc: 0.5172\n",
      "Epoch 6/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6436 - auc: 0.4952 - val_loss: 0.6461 - val_auc: 0.5210\n",
      "Epoch 7/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6424 - auc: 0.5018 - val_loss: 0.6458 - val_auc: 0.5185\n",
      "Epoch 8/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6434 - auc: 0.5045 - val_loss: 0.6457 - val_auc: 0.4990\n",
      "Epoch 9/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6424 - auc: 0.5092 - val_loss: 0.6462 - val_auc: 0.5073\n",
      "Epoch 10/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5085 - val_loss: 0.6458 - val_auc: 0.5203\n",
      "Epoch 11/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5072 - val_loss: 0.6488 - val_auc: 0.5191\n",
      "Epoch 12/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6430 - auc: 0.5042 - val_loss: 0.6454 - val_auc: 0.5193\n",
      "Epoch 13/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6424 - auc: 0.5060 - val_loss: 0.6494 - val_auc: 0.5168\n",
      "Epoch 14/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6426 - auc: 0.4975 - val_loss: 0.6457 - val_auc: 0.5147\n",
      "Epoch 15/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.5056 - val_loss: 0.6453 - val_auc: 0.5162\n",
      "Epoch 16/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6426 - auc: 0.4992 - val_loss: 0.6472 - val_auc: 0.5166\n",
      "Epoch 17/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6424 - auc: 0.5002 - val_loss: 0.6451 - val_auc: 0.5126\n",
      "Epoch 18/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5105 - val_loss: 0.6458 - val_auc: 0.5207\n",
      "Epoch 19/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6425 - auc: 0.5037 - val_loss: 0.6457 - val_auc: 0.5166\n",
      "Epoch 20/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.5009 - val_loss: 0.6459 - val_auc: 0.5229\n",
      "Epoch 21/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5063 - val_loss: 0.6482 - val_auc: 0.5160\n",
      "Epoch 22/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5068 - val_loss: 0.6454 - val_auc: 0.5146\n",
      "Epoch 23/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6424 - auc: 0.5043 - val_loss: 0.6451 - val_auc: 0.5157\n",
      "Epoch 24/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6430 - auc: 0.5006 - val_loss: 0.6515 - val_auc: 0.5185\n",
      "Epoch 25/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6420 - auc: 0.5062 - val_loss: 0.6453 - val_auc: 0.5156\n",
      "Epoch 26/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6422 - auc: 0.5023 - val_loss: 0.6450 - val_auc: 0.5209\n",
      "Epoch 27/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6422 - auc: 0.5018 - val_loss: 0.6477 - val_auc: 0.5164\n",
      "Epoch 28/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6418 - auc: 0.5026 - val_loss: 0.6469 - val_auc: 0.5168\n",
      "Epoch 29/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5055 - val_loss: 0.6470 - val_auc: 0.5197\n",
      "Epoch 30/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6415 - auc: 0.5061 - val_loss: 0.6489 - val_auc: 0.5156\n",
      "Epoch 31/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6428 - auc: 0.5012 - val_loss: 0.6452 - val_auc: 0.5155\n",
      "Epoch 32/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6420 - auc: 0.4948 - val_loss: 0.6449 - val_auc: 0.5218\n",
      "Epoch 33/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6428 - auc: 0.5016 - val_loss: 0.6456 - val_auc: 0.5181\n",
      "Epoch 34/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6422 - auc: 0.5074 - val_loss: 0.6454 - val_auc: 0.5137\n",
      "Epoch 35/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6415 - auc: 0.5056 - val_loss: 0.6483 - val_auc: 0.5206\n",
      "Epoch 36/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6436 - auc: 0.4959 - val_loss: 0.6476 - val_auc: 0.5151\n",
      "Epoch 37/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6424 - auc: 0.5004 - val_loss: 0.6470 - val_auc: 0.5207\n",
      "Epoch 38/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5052 - val_loss: 0.6461 - val_auc: 0.5181\n",
      "Epoch 39/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.5022 - val_loss: 0.6469 - val_auc: 0.5216\n",
      "Epoch 40/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5058 - val_loss: 0.6449 - val_auc: 0.5228\n",
      "Epoch 41/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6416 - auc: 0.5002 - val_loss: 0.6464 - val_auc: 0.5228\n",
      "Epoch 42/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6417 - auc: 0.5060 - val_loss: 0.6480 - val_auc: 0.5153\n",
      "Epoch 43/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6420 - auc: 0.5018 - val_loss: 0.6450 - val_auc: 0.5183\n",
      "Epoch 44/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6426 - auc: 0.4999 - val_loss: 0.6457 - val_auc: 0.5190\n",
      "Epoch 45/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6426 - auc: 0.5019 - val_loss: 0.6450 - val_auc: 0.5216\n",
      "Epoch 46/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6420 - auc: 0.5021 - val_loss: 0.6473 - val_auc: 0.5174\n",
      "Epoch 47/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6435 - auc: 0.4988 - val_loss: 0.6452 - val_auc: 0.5177\n",
      "Epoch 48/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6425 - auc: 0.5012 - val_loss: 0.6450 - val_auc: 0.5172\n",
      "Epoch 49/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5032 - val_loss: 0.6461 - val_auc: 0.5183\n",
      "Epoch 50/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6415 - auc: 0.5090 - val_loss: 0.6454 - val_auc: 0.5197\n",
      "Epoch 51/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.5001 - val_loss: 0.6525 - val_auc: 0.5204\n",
      "Epoch 52/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6427 - auc: 0.5060 - val_loss: 0.6450 - val_auc: 0.5203\n",
      "Epoch 53/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5047 - val_loss: 0.6462 - val_auc: 0.5183\n",
      "Epoch 54/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6434 - auc: 0.4974 - val_loss: 0.6451 - val_auc: 0.5212\n",
      "Epoch 55/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6413 - auc: 0.5085 - val_loss: 0.6451 - val_auc: 0.5227\n",
      "Epoch 56/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6418 - auc: 0.5069 - val_loss: 0.6453 - val_auc: 0.5187\n",
      "Epoch 57/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.5022 - val_loss: 0.6519 - val_auc: 0.5212\n",
      "Epoch 58/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6413 - auc: 0.5088 - val_loss: 0.6455 - val_auc: 0.5197\n",
      "Epoch 59/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6433 - auc: 0.5088 - val_loss: 0.6483 - val_auc: 0.5217\n",
      "Epoch 60/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.5059 - val_loss: 0.6453 - val_auc: 0.5155\n",
      "Epoch 61/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.4990 - val_loss: 0.6449 - val_auc: 0.5202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6429 - auc: 0.5028 - val_loss: 0.6451 - val_auc: 0.5159\n",
      "Epoch 63/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6412 - auc: 0.5090 - val_loss: 0.6449 - val_auc: 0.5185\n",
      "Epoch 64/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.5091 - val_loss: 0.6484 - val_auc: 0.5205\n",
      "Epoch 65/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6428 - auc: 0.5022 - val_loss: 0.6468 - val_auc: 0.5199\n",
      "Epoch 66/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6420 - auc: 0.5062 - val_loss: 0.6464 - val_auc: 0.5183\n",
      "Epoch 67/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6415 - auc: 0.5034 - val_loss: 0.6455 - val_auc: 0.5145\n",
      "Epoch 68/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.5057 - val_loss: 0.6457 - val_auc: 0.5217\n",
      "Epoch 69/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6418 - auc: 0.5056 - val_loss: 0.6460 - val_auc: 0.5150\n",
      "Epoch 70/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.4997 - val_loss: 0.6451 - val_auc: 0.5148\n",
      "Epoch 71/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6418 - auc: 0.5051 - val_loss: 0.6460 - val_auc: 0.5167\n",
      "Epoch 72/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6425 - auc: 0.5039 - val_loss: 0.6449 - val_auc: 0.5145\n",
      "Epoch 73/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6418 - auc: 0.5053 - val_loss: 0.6489 - val_auc: 0.5144\n",
      "Epoch 74/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6417 - auc: 0.4994 - val_loss: 0.6524 - val_auc: 0.5222\n",
      "Epoch 75/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6424 - auc: 0.5038 - val_loss: 0.6488 - val_auc: 0.5184\n",
      "Epoch 76/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.4982 - val_loss: 0.6454 - val_auc: 0.5195\n",
      "Epoch 77/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6416 - auc: 0.5040 - val_loss: 0.6471 - val_auc: 0.5212\n",
      "Epoch 78/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6420 - auc: 0.5056 - val_loss: 0.6450 - val_auc: 0.5194\n",
      "Epoch 79/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6418 - auc: 0.5070 - val_loss: 0.6450 - val_auc: 0.5167\n",
      "Epoch 80/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6414 - auc: 0.5119 - val_loss: 0.6453 - val_auc: 0.5181\n",
      "Epoch 81/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6436 - auc: 0.4986 - val_loss: 0.6518 - val_auc: 0.5098\n",
      "Epoch 82/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6425 - auc: 0.5068 - val_loss: 0.6507 - val_auc: 0.5200\n",
      "Epoch 83/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6433 - auc: 0.4946 - val_loss: 0.6455 - val_auc: 0.5184\n",
      "Epoch 84/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6423 - auc: 0.5056 - val_loss: 0.6452 - val_auc: 0.5184\n",
      "Epoch 85/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6412 - auc: 0.5109 - val_loss: 0.6481 - val_auc: 0.5215\n",
      "Epoch 86/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6418 - auc: 0.5056 - val_loss: 0.6476 - val_auc: 0.5206\n",
      "Epoch 87/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6423 - auc: 0.5049 - val_loss: 0.6450 - val_auc: 0.5129\n",
      "Epoch 88/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6418 - auc: 0.5056 - val_loss: 0.6468 - val_auc: 0.5208\n",
      "Epoch 89/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6415 - auc: 0.5103 - val_loss: 0.6450 - val_auc: 0.5176\n",
      "Epoch 90/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5033 - val_loss: 0.6449 - val_auc: 0.5214\n",
      "Epoch 91/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6422 - auc: 0.5034 - val_loss: 0.6450 - val_auc: 0.5204\n",
      "Epoch 92/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6419 - auc: 0.5062 - val_loss: 0.6473 - val_auc: 0.5205\n",
      "Epoch 93/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6418 - auc: 0.5118 - val_loss: 0.6540 - val_auc: 0.5001\n",
      "Epoch 94/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6423 - auc: 0.5002 - val_loss: 0.6484 - val_auc: 0.5184\n",
      "Epoch 95/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6428 - auc: 0.4936 - val_loss: 0.6450 - val_auc: 0.5173\n",
      "Epoch 96/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6425 - auc: 0.5020 - val_loss: 0.6450 - val_auc: 0.5175\n",
      "Epoch 97/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6421 - auc: 0.5032 - val_loss: 0.6474 - val_auc: 0.5190\n",
      "Epoch 98/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6418 - auc: 0.5010 - val_loss: 0.6458 - val_auc: 0.5169\n",
      "Epoch 99/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6430 - auc: 0.5022 - val_loss: 0.6450 - val_auc: 0.5227\n",
      "Epoch 100/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6411 - auc: 0.5117 - val_loss: 0.6487 - val_auc: 0.5208\n"
     ]
    }
   ],
   "source": [
    "allModels_1057, r_1057, model_1057 = GridSearch(xtrain_1057, ytrain_1057, param_list, cv=3, rand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2688e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1057 = model_1057.predict_classes(xtest_1057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fbd1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras_1057 = model_1057.predict(xtest_1057).ravel()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(ytest_1057, y_pred_keras_1057)\n",
    "auc_keras_1057 = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eea62b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras 597 (area = {:.3f})'.format(auc_keras_597))\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras 1057 (area = {:.3f})'.format(auc_keras_1057))\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65af55",
   "metadata": {},
   "source": [
    "# 4 - NN_Bilanciato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5da374dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1977/1977 [==============================] - 3s 2ms/step - loss: 0.6934 - auc: 0.4941 - val_loss: 0.6931 - val_auc: 0.5018\n",
      "Epoch 2/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6933 - auc: 0.4940 - val_loss: 0.6931 - val_auc: 0.5039\n",
      "Epoch 3/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6933 - auc: 0.4947 - val_loss: 0.6931 - val_auc: 0.5050\n",
      "Epoch 4/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6933 - auc: 0.4952 - val_loss: 0.6930 - val_auc: 0.5051\n",
      "Epoch 5/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6933 - auc: 0.4975 - val_loss: 0.6930 - val_auc: 0.5026\n",
      "Epoch 6/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6932 - auc: 0.4965 - val_loss: 0.6930 - val_auc: 0.5070\n",
      "Epoch 7/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6932 - auc: 0.4954 - val_loss: 0.6930 - val_auc: 0.5062\n",
      "Epoch 8/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6932 - auc: 0.4935 - val_loss: 0.6930 - val_auc: 0.5033\n",
      "Epoch 9/30\n",
      "1977/1977 [==============================] - 3s 2ms/step - loss: 0.6932 - auc: 0.4969 - val_loss: 0.6930 - val_auc: 0.5065\n",
      "Epoch 10/30\n",
      "1977/1977 [==============================] - 3s 2ms/step - loss: 0.6932 - auc: 0.4989 - val_loss: 0.6930 - val_auc: 0.5079\n",
      "Epoch 11/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6932 - auc: 0.5004 - val_loss: 0.6930 - val_auc: 0.5044\n",
      "Epoch 12/30\n",
      "1977/1977 [==============================] - 3s 2ms/step - loss: 0.6932 - auc: 0.5006 - val_loss: 0.6929 - val_auc: 0.5079\n",
      "Epoch 13/30\n",
      "1977/1977 [==============================] - 3s 2ms/step - loss: 0.6932 - auc: 0.5005 - val_loss: 0.6929 - val_auc: 0.5047\n",
      "Epoch 14/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6932 - auc: 0.4988 - val_loss: 0.6929 - val_auc: 0.5088\n",
      "Epoch 15/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6931 - auc: 0.4990 - val_loss: 0.6929 - val_auc: 0.5104\n",
      "Epoch 16/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6931 - auc: 0.4993 - val_loss: 0.6929 - val_auc: 0.5076\n",
      "Epoch 17/30\n",
      "1977/1977 [==============================] - 3s 2ms/step - loss: 0.6931 - auc: 0.5009 - val_loss: 0.6929 - val_auc: 0.5100\n",
      "Epoch 18/30\n",
      "1977/1977 [==============================] - 3s 2ms/step - loss: 0.6931 - auc: 0.5032 - val_loss: 0.6929 - val_auc: 0.5088\n",
      "Epoch 19/30\n",
      "1977/1977 [==============================] - 3s 2ms/step - loss: 0.6931 - auc: 0.5029 - val_loss: 0.6929 - val_auc: 0.5091\n",
      "Epoch 20/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6931 - auc: 0.5018 - val_loss: 0.6929 - val_auc: 0.5100\n",
      "Epoch 21/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6931 - auc: 0.5043 - val_loss: 0.6929 - val_auc: 0.5100\n",
      "Epoch 22/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6931 - auc: 0.5059 - val_loss: 0.6929 - val_auc: 0.5106\n",
      "Epoch 23/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6931 - auc: 0.5052 - val_loss: 0.6929 - val_auc: 0.5104\n",
      "Epoch 24/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6931 - auc: 0.5055 - val_loss: 0.6929 - val_auc: 0.5098\n",
      "Epoch 25/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6931 - auc: 0.5047 - val_loss: 0.6929 - val_auc: 0.5108\n",
      "Epoch 26/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6931 - auc: 0.5054 - val_loss: 0.6929 - val_auc: 0.5108\n",
      "Epoch 27/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6931 - auc: 0.5057 - val_loss: 0.6929 - val_auc: 0.5116\n",
      "Epoch 28/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6930 - auc: 0.5058 - val_loss: 0.6929 - val_auc: 0.5100\n",
      "Epoch 29/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6930 - auc: 0.5071 - val_loss: 0.6929 - val_auc: 0.5132\n",
      "Epoch 30/30\n",
      "1977/1977 [==============================] - 3s 1ms/step - loss: 0.6930 - auc: 0.5073 - val_loss: 0.6929 - val_auc: 0.5106\n",
      "Epoch 1/30\n",
      "396/396 [==============================] - 1s 3ms/step - loss: 0.6955 - auc: 0.5014 - val_loss: 0.6940 - val_auc: 0.5000\n",
      "Epoch 2/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6945 - auc: 0.4931 - val_loss: 0.6962 - val_auc: 0.5000\n",
      "Epoch 3/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6948 - auc: 0.4998 - val_loss: 0.6937 - val_auc: 0.5000\n",
      "Epoch 4/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6945 - auc: 0.4947 - val_loss: 0.6961 - val_auc: 0.5000\n",
      "Epoch 5/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6944 - auc: 0.4972 - val_loss: 0.6933 - val_auc: 0.5000\n",
      "Epoch 6/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6940 - auc: 0.5022 - val_loss: 0.6947 - val_auc: 0.5000\n",
      "Epoch 7/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6941 - auc: 0.5023 - val_loss: 0.6934 - val_auc: 0.5000\n",
      "Epoch 8/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6940 - auc: 0.5059 - val_loss: 0.6931 - val_auc: 0.5000\n",
      "Epoch 9/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6943 - auc: 0.4959 - val_loss: 0.6944 - val_auc: 0.5000\n",
      "Epoch 10/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6942 - auc: 0.5009 - val_loss: 0.6944 - val_auc: 0.5000\n",
      "Epoch 11/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6942 - auc: 0.5021 - val_loss: 0.6951 - val_auc: 0.5000\n",
      "Epoch 12/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6942 - auc: 0.4960 - val_loss: 0.6933 - val_auc: 0.5000\n",
      "Epoch 13/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6939 - auc: 0.5002 - val_loss: 0.6932 - val_auc: 0.5000\n",
      "Epoch 14/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6941 - auc: 0.4958 - val_loss: 0.6936 - val_auc: 0.5000\n",
      "Epoch 15/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6940 - auc: 0.5021 - val_loss: 0.6932 - val_auc: 0.5000\n",
      "Epoch 16/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6939 - auc: 0.4980 - val_loss: 0.6961 - val_auc: 0.5000\n",
      "Epoch 17/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6942 - auc: 0.4971 - val_loss: 0.6937 - val_auc: 0.5000\n",
      "Epoch 18/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6942 - auc: 0.4960 - val_loss: 0.6957 - val_auc: 0.5000\n",
      "Epoch 19/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6945 - auc: 0.4927 - val_loss: 0.6958 - val_auc: 0.5000\n",
      "Epoch 20/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6943 - auc: 0.5006 - val_loss: 0.6935 - val_auc: 0.5000\n",
      "Epoch 21/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6939 - auc: 0.5022 - val_loss: 0.6953 - val_auc: 0.5137\n",
      "Epoch 22/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6943 - auc: 0.4949 - val_loss: 0.6932 - val_auc: 0.5000\n",
      "Epoch 23/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6946 - auc: 0.4940 - val_loss: 0.6932 - val_auc: 0.5000\n",
      "Epoch 24/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6945 - auc: 0.4966 - val_loss: 0.6935 - val_auc: 0.5084\n",
      "Epoch 25/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6942 - auc: 0.4925 - val_loss: 0.6939 - val_auc: 0.5000\n",
      "Epoch 26/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6943 - auc: 0.4899 - val_loss: 0.6934 - val_auc: 0.5106\n",
      "Epoch 27/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6946 - auc: 0.5010 - val_loss: 0.6935 - val_auc: 0.5000\n",
      "Epoch 28/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6946 - auc: 0.4984 - val_loss: 0.6932 - val_auc: 0.5000\n",
      "Epoch 29/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6946 - auc: 0.4969 - val_loss: 0.6943 - val_auc: 0.5000\n",
      "Epoch 30/30\n",
      "396/396 [==============================] - 1s 2ms/step - loss: 0.6939 - auc: 0.5031 - val_loss: 0.6934 - val_auc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model1_sb_1057 = build_model_1(xtrain_sb_1057)\n",
    "history1_sb_1057 = model1_sb_1057.fit(xtrain_sb_1057, ytrain_sb_1057, validation_data=(xtest_sb_1057, ytest_sb_1057), epochs=30, batch_size=10).history\n",
    "\n",
    "model2_sb_1057 = build_model_2(xtrain_sb_1057)\n",
    "history2_sb_1057 = model2_sb_1057.fit(xtrain_sb_1057, ytrain_sb_1057, validation_data=(xtest_sb_1057, ytest_sb_1057), epochs=30, batch_size=50).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e1b1261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/265 [==============================] - 0s 867us/step - loss: 0.6929 - auc: 0.5106\n",
      "265/265 [==============================] - 0s 947us/step - loss: 0.6934 - auc: 0.5000\n",
      "Loss 0.692858, AUC 0.510615\n",
      "Loss 0.693363, AUC 0.500000\n"
     ]
    }
   ],
   "source": [
    "test_loss_1_sb_1057, test_auc_1_sb_1057 = model1_sb_1057.evaluate(xtest_sb_1057, ytest_sb_1057)\n",
    "test_loss_2_sb_1057, test_auc_2_sb_1057 = model2_sb_1057.evaluate(xtest_sb_1057, ytest_sb_1057)\n",
    "\n",
    "print('Loss %f, AUC %f' % (test_loss_1_sb_1057, test_auc_1_sb_1057))\n",
    "print('Loss %f, AUC %f' % (test_loss_2_sb_1057, test_auc_2_sb_1057))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b656feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (128, 100, 0.261, 0.9, False, 'tanh', 0.0001, 10, 1)\n",
      "Epoch 1/100\n",
      "103/103 [==============================] - 1s 5ms/step - loss: 0.6992 - auc: 0.5023 - val_loss: 0.6968 - val_auc: 0.5120\n",
      "Epoch 2/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6961 - auc: 0.4948 - val_loss: 0.6953 - val_auc: 0.5109\n",
      "Epoch 3/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6956 - auc: 0.5048 - val_loss: 0.6950 - val_auc: 0.5093\n",
      "Epoch 4/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6956 - auc: 0.4970 - val_loss: 0.6946 - val_auc: 0.5112\n",
      "Epoch 5/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6952 - auc: 0.5047 - val_loss: 0.6955 - val_auc: 0.5030\n",
      "Epoch 6/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6949 - auc: 0.5048 - val_loss: 0.6944 - val_auc: 0.5025\n",
      "Epoch 7/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6950 - auc: 0.5012 - val_loss: 0.6945 - val_auc: 0.5024\n",
      "Epoch 8/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6949 - auc: 0.4988 - val_loss: 0.6943 - val_auc: 0.5053\n",
      "Epoch 9/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.5026 - val_loss: 0.6939 - val_auc: 0.5068\n",
      "Epoch 10/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6952 - auc: 0.4999 - val_loss: 0.6972 - val_auc: 0.5118\n",
      "Epoch 11/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6953 - auc: 0.4976 - val_loss: 0.6957 - val_auc: 0.5030\n",
      "Epoch 12/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6945 - auc: 0.4987 - val_loss: 0.7076 - val_auc: 0.5051\n",
      "Epoch 13/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6953 - auc: 0.5003 - val_loss: 0.6976 - val_auc: 0.5045\n",
      "Epoch 14/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6959 - auc: 0.4912 - val_loss: 0.6939 - val_auc: 0.5095\n",
      "Epoch 15/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6953 - auc: 0.5058 - val_loss: 0.6936 - val_auc: 0.5151\n",
      "Epoch 16/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.5116 - val_loss: 0.6936 - val_auc: 0.5048\n",
      "Epoch 17/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6949 - auc: 0.4977 - val_loss: 0.6936 - val_auc: 0.5063\n",
      "Epoch 18/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6944 - auc: 0.5027 - val_loss: 0.6942 - val_auc: 0.5034\n",
      "Epoch 19/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6946 - auc: 0.5003 - val_loss: 0.6935 - val_auc: 0.5139\n",
      "Epoch 20/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6937 - auc: 0.5071 - val_loss: 0.6935 - val_auc: 0.5157\n",
      "Epoch 21/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6965 - auc: 0.4966 - val_loss: 0.6945 - val_auc: 0.5026\n",
      "Epoch 22/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6944 - auc: 0.4970 - val_loss: 0.6938 - val_auc: 0.5051\n",
      "Epoch 23/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6945 - auc: 0.5106 - val_loss: 0.6958 - val_auc: 0.5157\n",
      "Epoch 24/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6954 - auc: 0.4947 - val_loss: 0.6949 - val_auc: 0.5090\n",
      "Epoch 25/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6948 - auc: 0.4965 - val_loss: 0.6942 - val_auc: 0.5044\n",
      "Epoch 26/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6941 - auc: 0.5028 - val_loss: 0.6941 - val_auc: 0.5158\n",
      "Epoch 27/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6950 - auc: 0.5021 - val_loss: 0.6935 - val_auc: 0.5155\n",
      "Epoch 28/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6963 - auc: 0.5015 - val_loss: 0.6972 - val_auc: 0.5116\n",
      "Epoch 29/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6950 - auc: 0.5025 - val_loss: 0.6936 - val_auc: 0.5118\n",
      "Epoch 30/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6945 - auc: 0.4967 - val_loss: 0.6968 - val_auc: 0.5138\n",
      "Epoch 31/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6954 - auc: 0.4987 - val_loss: 0.6952 - val_auc: 0.5105\n",
      "Epoch 32/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6948 - auc: 0.5008 - val_loss: 0.6935 - val_auc: 0.5073\n",
      "Epoch 33/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6942 - auc: 0.5048 - val_loss: 0.6951 - val_auc: 0.5148\n",
      "Epoch 34/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6937 - auc: 0.5044 - val_loss: 0.6935 - val_auc: 0.5184\n",
      "Epoch 35/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6940 - auc: 0.5037 - val_loss: 0.6944 - val_auc: 0.5083\n",
      "Epoch 36/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6940 - auc: 0.5030 - val_loss: 0.6933 - val_auc: 0.5061\n",
      "Epoch 37/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6940 - auc: 0.5080 - val_loss: 0.6932 - val_auc: 0.5079\n",
      "Epoch 38/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6945 - auc: 0.5014 - val_loss: 0.6939 - val_auc: 0.5130\n",
      "Epoch 39/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.4916 - val_loss: 0.6959 - val_auc: 0.5107\n",
      "Epoch 40/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6950 - auc: 0.5054 - val_loss: 0.6932 - val_auc: 0.5092\n",
      "Epoch 41/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.5053 - val_loss: 0.6981 - val_auc: 0.5090\n",
      "Epoch 42/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6954 - auc: 0.4997 - val_loss: 0.6935 - val_auc: 0.5061\n",
      "Epoch 43/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6950 - auc: 0.4976 - val_loss: 0.6970 - val_auc: 0.5033\n",
      "Epoch 44/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6941 - auc: 0.4947 - val_loss: 0.6934 - val_auc: 0.5056\n",
      "Epoch 45/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6940 - auc: 0.4997 - val_loss: 0.6937 - val_auc: 0.5148\n",
      "Epoch 46/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6944 - auc: 0.5057 - val_loss: 0.6947 - val_auc: 0.5065\n",
      "Epoch 47/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.5005 - val_loss: 0.6932 - val_auc: 0.5059\n",
      "Epoch 48/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6946 - auc: 0.4966 - val_loss: 0.6949 - val_auc: 0.5196\n",
      "Epoch 49/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6951 - auc: 0.4951 - val_loss: 0.6949 - val_auc: 0.5078\n",
      "Epoch 50/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6941 - auc: 0.4977 - val_loss: 0.6943 - val_auc: 0.5184\n",
      "Epoch 51/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6939 - auc: 0.5056 - val_loss: 0.6937 - val_auc: 0.5084\n",
      "Epoch 52/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6938 - auc: 0.5073 - val_loss: 0.6954 - val_auc: 0.5177\n",
      "Epoch 53/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6944 - auc: 0.5011 - val_loss: 0.6935 - val_auc: 0.5100\n",
      "Epoch 54/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6938 - auc: 0.5047 - val_loss: 0.6946 - val_auc: 0.5152\n",
      "Epoch 55/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6954 - auc: 0.4981 - val_loss: 0.6936 - val_auc: 0.5152\n",
      "Epoch 56/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6945 - auc: 0.5011 - val_loss: 0.6935 - val_auc: 0.5191\n",
      "Epoch 57/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6935 - auc: 0.5097 - val_loss: 0.6932 - val_auc: 0.5150\n",
      "Epoch 58/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6940 - auc: 0.5026 - val_loss: 0.6941 - val_auc: 0.5049\n",
      "Epoch 59/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6948 - auc: 0.4917 - val_loss: 0.6932 - val_auc: 0.5083\n",
      "Epoch 60/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6946 - auc: 0.4977 - val_loss: 0.6946 - val_auc: 0.5052\n",
      "Epoch 61/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6944 - auc: 0.5075 - val_loss: 0.7063 - val_auc: 0.5157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6964 - auc: 0.4938 - val_loss: 0.6947 - val_auc: 0.5057\n",
      "Epoch 63/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6931 - auc: 0.5165 - val_loss: 0.6969 - val_auc: 0.5137\n",
      "Epoch 64/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6938 - auc: 0.5084 - val_loss: 0.6943 - val_auc: 0.5059\n",
      "Epoch 65/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.5003 - val_loss: 0.6945 - val_auc: 0.5153\n",
      "Epoch 66/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6948 - auc: 0.4977 - val_loss: 0.6941 - val_auc: 0.5054\n",
      "Epoch 67/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.4968 - val_loss: 0.6962 - val_auc: 0.5185\n",
      "Epoch 68/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6937 - auc: 0.5083 - val_loss: 0.6934 - val_auc: 0.5110\n",
      "Epoch 69/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6952 - auc: 0.4964 - val_loss: 0.6932 - val_auc: 0.5142\n",
      "Epoch 70/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6934 - auc: 0.5083 - val_loss: 0.6933 - val_auc: 0.5084\n",
      "Epoch 71/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6945 - auc: 0.5011 - val_loss: 0.6942 - val_auc: 0.5183\n",
      "Epoch 72/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6942 - auc: 0.4995 - val_loss: 0.6971 - val_auc: 0.5146\n",
      "Epoch 73/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6946 - auc: 0.5016 - val_loss: 0.6933 - val_auc: 0.5203\n",
      "Epoch 74/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6936 - auc: 0.5031 - val_loss: 0.6934 - val_auc: 0.5076\n",
      "Epoch 75/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6942 - auc: 0.5014 - val_loss: 0.6956 - val_auc: 0.5148\n",
      "Epoch 76/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6958 - auc: 0.4949 - val_loss: 0.6944 - val_auc: 0.5150\n",
      "Epoch 77/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6938 - auc: 0.5047 - val_loss: 0.6959 - val_auc: 0.5069\n",
      "Epoch 78/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6944 - auc: 0.5009 - val_loss: 0.6932 - val_auc: 0.5179\n",
      "Epoch 79/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6946 - auc: 0.5045 - val_loss: 0.6932 - val_auc: 0.5111\n",
      "Epoch 80/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.4937 - val_loss: 0.6940 - val_auc: 0.5174\n",
      "Epoch 81/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6944 - auc: 0.4939 - val_loss: 0.6939 - val_auc: 0.5174\n",
      "Epoch 82/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.5007 - val_loss: 0.6934 - val_auc: 0.5183\n",
      "Epoch 83/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6935 - auc: 0.5127 - val_loss: 0.6949 - val_auc: 0.5178\n",
      "Epoch 84/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6935 - auc: 0.5094 - val_loss: 0.6946 - val_auc: 0.5099\n",
      "Epoch 85/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6936 - auc: 0.5079 - val_loss: 0.6933 - val_auc: 0.5068\n",
      "Epoch 86/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6941 - auc: 0.5062 - val_loss: 0.6932 - val_auc: 0.5074\n",
      "Epoch 87/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6940 - auc: 0.4989 - val_loss: 0.6942 - val_auc: 0.5083\n",
      "Epoch 88/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6939 - auc: 0.4998 - val_loss: 0.6943 - val_auc: 0.5085\n",
      "Epoch 89/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6948 - auc: 0.4994 - val_loss: 0.6976 - val_auc: 0.5065\n",
      "Epoch 90/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6952 - auc: 0.4899 - val_loss: 0.6933 - val_auc: 0.5069\n",
      "Epoch 91/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6938 - auc: 0.5046 - val_loss: 0.7026 - val_auc: 0.5103\n",
      "Epoch 92/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6965 - auc: 0.4968 - val_loss: 0.6934 - val_auc: 0.5017\n",
      "Epoch 93/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6935 - auc: 0.5092 - val_loss: 0.6932 - val_auc: 0.5161\n",
      "Epoch 94/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6944 - auc: 0.4989 - val_loss: 0.6934 - val_auc: 0.5087\n",
      "Epoch 95/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.5020 - val_loss: 0.6946 - val_auc: 0.5199\n",
      "Epoch 96/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6941 - auc: 0.5055 - val_loss: 0.6967 - val_auc: 0.5066\n",
      "Epoch 97/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6947 - auc: 0.4985 - val_loss: 0.6937 - val_auc: 0.5111\n",
      "Epoch 98/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6943 - auc: 0.5002 - val_loss: 0.6960 - val_auc: 0.5089\n",
      "Epoch 99/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6945 - auc: 0.5052 - val_loss: 0.6968 - val_auc: 0.5059\n",
      "Epoch 100/100\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.6934 - auc: 0.5137 - val_loss: 0.6934 - val_auc: 0.5135\n"
     ]
    }
   ],
   "source": [
    "allModels_sb_1057, r_sb_1057, model_sb_1057 = GridSearch(xtrain_sb_1057, ytrain_sb_1057, param_list, cv=3, rand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a78aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sb_1057 = model_sb_1057.predict_classes(xtest_sb_1057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7703f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras_sb_1057 = model_sb_1057.predict(xtest_sb_1057).ravel()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(ytest_sb_1057, y_pred_keras_sb_1057)\n",
    "auc_keras_sb_1057 = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3a0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6591e1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3xN5x/A8c8jIRGxY8feCWJkaqk9fyWqiqpVo6iq6qDDaGnNlipqVpQapStFzVKKRGIThKLETuxIZNzn98eN24ybiHFzM77v1+u+5NzznHO+103u957nOef7KK01Qgghcq5c1g5ACCGEdUkiEEKIHE4SgRBC5HCSCIQQIoeTRCCEEDmcJAIhhMjhJBEIIUQOJ4lAZDtKqXNKqSil1D2l1BWllJ9SyjFZm4ZKqT+VUneVUreVUr8rpVyStSmglJqhlDqfsK/TCctOGfuKhLAsSQQiu3pRa+0I1AXqAR8+XKGU8gE2Ab8BpYGKwCFgl1KqUkKbPMBWwBVoAxQAGgIRgKelglZK2Vpq30KkRhKByNa01leAjRgTwkNTgO+11l9rre9qrW9orT8BAoBxCW16AeWATlrrEK21QWt9TWs9Xmu93tyxlFKuSqnNSqkbSqmrSqmPEp73U0pNSNSuiVIqLNHyOaXUSKXUYSBSKfWJUmpNsn1/rZSamfBzQaXUIqXUZaXURaXUBKWUzVP+V4kcTBKByNaUUs5AW+B0wrIDxm/2q800/xFomfBzC2CD1vpeOo+TH9gCbMB4llEF4xlFenUH2gOFgKVAO6VUgYR92wCvAMsT2i4B4hKOUQ9oBfR/jGMJkYQkApFd/aqUugtcAK4BYxOeL4Lx9/6ymW0uAw/7/4um0iY1/wOuaK2/1FpHJ5xpBD7G9jO11he01lFa63+B/YBvwrpmwH2tdYBSqgTGxDZcax2ptb4GTAe6PcaxhEhCEoHIrny11vmBJkAN/vuAvwkYgFJmtikFhCf8HJFKm9SUBf55okiNLiRbXo7xLAHgVf47GygP5AYuK6VuKaVuAfOA4k9xbJHDSSIQ2ZrW+i/AD5iWsBwJ7AG6mGn+Cv9152wBWiul8qXzUBeAyqmsiwQcEi2XNBdqsuXVQJOErq1O/JcILgAPACetdaGERwGttWs64xQiBUkEIieYAbRUSj0cMB4F9FZKDVNK5VdKFU4YzPUBPk1osxTjh+5PSqkaSqlcSqmiSqmPlFLtzBxjLVBSKTVcKWWXsF+vhHUHMfb5F1FKlQSGPypgrfV1YDuwGDirtT6e8PxljFc8fZlweWsupVRlpdQLT/D/IgQgiUDkAAkfqt8DoxOW/wZaAy9hHAf4F+Og6/Na61MJbR5gHDA+AWwG7gB7MXYxpej711rfxTjQ/CJwBTgFNE1YvRTj5annMH6Ir0pn6MsTYlie7PleQB4gBGNX1xoerxtLiCSUTEwjhBA5m5wRCCFEDieJQAghcjhJBEIIkcNJIhBCiBwuyxW4cnJy0hUqVLB2GEIIkaXs27cvXGtdzNy6LJcIKlSoQHBwsLXDEEKILEUp9W9q66RrSAghcjhJBEIIkcNJIhBCiBwuy40RmBMbG0tYWBjR0dHWDkWIbMne3h5nZ2dy585t7VCEBWSLRBAWFkb+/PmpUKECSilrhyNEtqK1JiIigrCwMCpWrGjtcIQFWKxrSCn1nVLqmlLqaCrrlVJqZsKE4IeVUvWf9FjR0dEULVpUkoAQFqCUomjRonLGnY1ZcozAD+Ok36lpC1RNeAwEvn2ag0kSEMJy5O8re7NYItBa7wBupNGkI8YJxLXWOgAopJSSUrpCCJHM6cN72Dp7GEd2/GKR/VtzjKAMSafnC0t4LsU8sUqpgRjPGihXrlyGBCeEENakDQYCFr6N58WlVFGaKsCuY/HQuNMzP5Y1Lx81d65pdnIErfV8rbW71tq9WDGzd0hbnaOjo+nn9evXU7VqVc6fP2/x4/r5+TF06FCz6xo2bAjAuXPnqFWrlsVjeXis5cuTz6PyaDY2NtStW5e6devSoUMH0/N//vkn9evXp1atWvTu3Zu4uDgApk6dampfq1YtbGxsuHEj5Qmo1ppmzZpx586dJ39RFrZkyRKqVq1K1apVWbJkidk2fn5+FCtWzPSaFy5cmGT9nTt3KFOmTJLfha1bt1K/fn3q1q3L888/z+nTpwFYu3YtY8eOtdwLEk8lNuYBexaP5Mpn1fC59D02SvNPXDFWOH/Kc4NnW+agWmuLPYAKwNFU1s0DuidaPgmUetQ+GzRooJMLCQlJ8VxGy5cvn9Za6y1btuhKlSrp06dPp3vbuLi4Jz7u4sWL9Ztvvplmm7Nnz2pXV9cnPsbj2LZtm27fvv1jb/fw/y+x+Ph47ezsrE+ePKm11nr06NF64cKFKdr5+/vrpk2bmt3v2rVr9fDhwx8rlqd5Px5XRESErlixoo6IiNA3btzQFStW1Ddu3EjR7lHv87Bhw3T37t2TtKlatarpb2P27Nm6d+/eWmutDQaDrlu3ro6MjHysWDPD31l2dv3iOR3wTV99flwNrccW0HpsAT2uewM9oF8fs78TjwsI1ql8rlqza8gfGKqUWgl4Abe1cT7Wp/Lp78cIufRsv/25lC7A2BcfPTf4zp07GTBgAOvXr6dyZeM85suWLWPmzJnExMTg5eXFnDlzsLGxwdHRkREjRrBx40a+/PJL/vzzT37//XeioqJo2LAh8+bNQynFzJkzmTt3Lra2tri4uLBy5coUx71w4QJt2rTh7NmzvPrqq6Zve46Ojty7dy9J23PnztGzZ08iIyMBmDVrFg0bNmT79u2MGzcOJycnjh49SoMGDVi2bBlKKYKCgnj77beJjIzEzs6OrVu3EhERYXY/o0aN4vjx49StW5fevXszePBgBg8eTHBwMLa2tnz11Vc0bdqU9IiIiMDOzo5q1aoB0LJlSyZOnEi/fv2StFuxYgXdu3c3u48ffviBgQMHmpZ9fX25cOEC0dHRvP3226Z1yd+Pc+fOmX3fBg8eTFBQEFFRUbz88st8+umnZo+bXhs3bqRly5YUKVLE9Bo3bNiQ6usxZ9++fVy9epU2bdokqcOllDKdCd2+fZvSpUubnm/SpAlr167llVdeear4xdOLjorkwNJReF1cipPS3MKRgKrvcq+ENw2fU7Rs2dLiMVgsESilVgBNACelVBgwFsgNoLWeC6wH2gGngftAX0vFkhEePHhAx44d2b59OzVq1ADg+PHjrFq1il27dpE7d26GDBnCDz/8QK9evYiMjKRWrVp89tlnALi4uDBmzBgAevbsydq1a3nxxReZNGkSZ8+exc7Ojlu3bpk99t69ezl69CgODg54eHjQvn173N3dzbYtXrw4mzdvxt7enlOnTtG9e3fTh8eBAwc4duwYpUuX5rnnnmPXrl14enrStWtXVq1ahYeHB3fu3CFv3ryp7mfSpElMmzaNtWvXAvDll18CcOTIEU6cOEGrVq0IDQ3F3t4+SVzR0dG4u7tja2vLqFGj8PX1xcnJidjYWIKDg3F3d2fNmjVcuHAhyXb3799nw4YNzJo1y+zr3bVrF/PmzTMtf/fddxQpUoSoqCg8PDzo3LkzRYsWTfJ+HD9+nMmTJ5t93z7//HOKFClCfHw8zZs35/Dhw9SpUyfJMadOncoPP/yQIpbGjRszc+bMJM9dvHiRsmXLmpadnZ25ePGi2dfy008/sWPHDqpVq8b06dMpW7YsBoOBd999l6VLl7J169Yk7RcuXEi7du3ImzcvBQoUICAgwLTO3d2dnTt3SiKwouj79ziyyY+KB6fiwy0CbN35auddqri34IseYzI0FoslAq11ml9pEk5V3nzWx03PN3dLyJ07Nw0bNmTRokV8/fXXgLGPdt++fXh4eAAQFRVF8eLFAWOfeOfOnU3bb9u2jSlTpnD//n1u3LiBq6srL774InXq1KFHjx74+vri6+tr9tgtW7akaNGiALz00kv8/fffqSaC2NhYhg4dysGDB7GxsSE0NNS0ztPTE2dnZwDq1q3LuXPnKFiwIKVKlTK9hgIFCgAQGRmZ6n4S+/vvv3nrrbcAqFGjBuXLlyc0NDTFh+f58+cpXbo0Z86coVmzZtSuXZvKlSuzcuVK3nnnHR48eECrVq2wtU36K/v777/z3HPPmb5RJ3fjxg3y589vWp45cya//GK88uLChQucOnWKokWLJnk/0nrffvzxR+bPn09cXByXL18mJCQkxWt5//33ef/9983Gk5w2M2e4uUs1X3zxRbp3746dnR1z586ld+/e/Pnnn8yZM4d27dolSSYPTZ8+nfXr1+Pl5cXUqVMZMWKEaWyhePHiXLp0KV0ximcveN0CKgd9hgd3uKfz8vXdFgyf/jM1atRg+Lj/ZXg82eLO4swgV65c/Pjjj7Ro0YIvvviCjz76CK01vXv3ZuLEiSna29vbY2NjAxi/DQ8ZMoTg4GDKli3LuHHjTDfvrFu3jh07duDv78/48eM5duxYig/D5B8caV3zPX36dEqUKMGhQ4cwGAxJvpnb2dmZfraxsSEuLg6ttdn9pbWfxMx90JnzsNuiUqVKNGnShAMHDlC5cmV8fHzYuXMnAJs2bUqRcFauXJlmN4qtrS0Gg4FcuXKxfft2tmzZwp49e3BwcKBJkyam/+fE70dq79vZs2eZNm0aQUFBFC5cmD59+pi9yepxzgicnZ3Zvn27aTksLIwmTZqk2PZhogcYMGAAI0eOBGDPnj3s3LmTOXPmcO/ePWJiYnB0dOTdd9/l0KFDeHl5AdC1a1fatPnvtp7o6Gjy5s2b6v+bsIyzxwIJ3/QlHrc3cpMC/FrmA4aOm83Va/58/PHHfPLJJ6n+LVmSFJ17hhwcHFi7di0//PADixYtonnz5qxZs4Zr164Bxm+n//6bsiT4ww8TJycn7t27x5o1awAwGAxcuHCBpk2bMmXKFG7dupWizx9g8+bN3Lhxg6ioKH799Veee+65VGO8ffs2pUqVIleuXCxdupT4+Pg0X1ONGjW4dOkSQUFBANy9e5e4uLhU95M/f37u3r1r2r5x48amD8XQ0FDOnz9P9erVkxzj5s2bPHjwAIDw8HB27dqFi4sLgOn/7sGDB0yePJlBgwYleS1//fUXHTt2TDX+6tWrc+bMGVP7woUL4+DgwIkTJ5J0lSSW2vt2584d8uXLR8GCBbl69Sp//PGH2e3ff/99Dh48mOKRPAkAtG7dmk2bNnHz5k1u3rzJpk2baN26dYp2ly//N3zm7+9PzZo1AeMYyPnz5zl37hzTpk2jV69eTJo0icKFC3P79m1T4ty8ebNpGzC+Fxl1JVlO9yD6Pof+/JHjn/tQcXUrPG5vJLBoR/K8e4TK3h0oW648QUFBTJgwwSpJAOSM4JkrUqQIGzZsoHHjxsyYMYMJEybQqlUrDAYDuXPnZvbs2ZQvXz7JNoUKFWLAgAHUrl2bChUqmLok4uPjee2117h9+zZaa9555x0KFSqU4pjPP/88PXv25PTp07z66qupdgsBDBkyhM6dO7N69WqaNm1Kvnz50nw9efLkYdWqVbz11ltERUWRN29etmzZkup+6tSpg62tLW5ubvTp04chQ4YwaNAgateuja2tLX5+fknOPMA4lvLGG2+QK1cuDAYDo0aNMiWCqVOnsnbtWgwGA4MHD6ZZs2am7X755RdatWqV5mto374927dvp0qVKrRp04a5c+dSp04dqlevjre3t9ltXFxczL5v3t7e1KtXD1dXVypVqpRmwk2vIkWKMHr0aNN7PmbMGFM315gxY3B3d6dDhw7MnDkTf39/bG1tKVKkCH5+fmnu19bWlgULFtC5c2dy5cpF4cKF+e6770zrt23bZvZMVTw7J4K3civoR7yvrsQt4bmAYl04pmpy6MS/zMxfiNq1C7F7926r37mt0nvqnlm4u7vr5DOUHT9+PMm3HSEeunz5Mr169WLz5s3WDiXTuHr1Kq+++mqKweVHkb+z9DkfepD7qwdTIzYEgBO2NblZ0ofcLi/y0bhJbN68mUaNGrFx48YM7Z5TSu3TWpv9lihdQyJbK1WqFAMGDMjUN5RltPPnz5uu5hLPjiE+nv3TXqTMD02oERvCsTxuXHk9mKof7iLofkVeaPkie/bsYc6cOWzfvj1TjdFI15DI9uQSyaQedkOJZ+PS2RNcWDeZkjf2Ut8Qxmnbyth3/x7XKsYxmKtXrzJmzBheeOEF5s6dmynL5EgiEEKIJ/Ag+j7HvnmF+pE7KQ2ctK3O3toTaPDiYAxa4+fnR69evShRogT79++nYsWKVh8LSI0kAiGEeExH//anwuaB1FdRnLapTEyLz3HxaQsY7/R+/fXXOXz4MKVKlaJ169ZUqlTJyhGnTRKBEEKk05mjgdj91JNa+irXVWFC3D7Gs5PxhsmoqCg+/fRTpk2bRvHixfnll1/MXgqcGUkiEEKIR9AGA4HzBuN91VjrK7hAC2oNWoKnw39Vh319fdm0aRP9+/dn6tSpZi/1zqzkqqFnRMpQYzrWk5ShbtOmDYUKFeJ//0t6e/3Zs2fx8vKiatWqdO3alZiYGAC2b99OwYIFTWWZH9ZsOnnypOm5unXrUqBAAWbMmGH2mDNmzOD7779/7FgzSmqvPbnUSnj36NGD6tWrU6tWLV5//XViY2OTbBcUFISNjY3pBsaYmBgaN25sKvUtjMIv/cuRKa1MSeBYy+W4j/gJewdH7ty5Y7oh9KOPPmLLli0sWLAgSyUBwLJlqC3xkDLUSWWXMtRbtmzR/v7+Kbbt0qWLXrFihdZa6zfeeEPPmTMn3ceJi4vTJUqU0OfOnUuxLjY2VteuXVvHxsamO8bHafsspPbakzNXwltrrdetW6cNBoM2GAy6W7duSbaPi4vTTZs21W3bttWrV682PT9u3Di9bNkys/vLDH9nGelsSJA+NqGhqSS0HltA37px3bR+3bp1umzZsvrDDz+0YpTpRxplqLPfGcEfo2Bx+2f7+GNUug79sAz1unXrkpSh9vT0pG7durzxxhumUgyOjo6MGTMGLy8v9uzZw2effYaHhwe1atVi4MCBpho9M2fOxMXFhTp16tCtWzezx31Yhrp69epJyiInPkt56Ny5czRq1Ij69etTv359du/eDRi/YTdp0oSXX36ZGjVq0KNHD1MMQUFBNGzYEDc3Nzw9Pbl7926q+xk1ahQ7d+6kbt26TJ8+nejoaPr27Uvt2rWpV68e27ZtM/samjdvnqQ4HBi/pPz555+8/PLLAPTu3Ztff/01Xe8FGIvHVa5cOcWd3PDfhDcP6zYtWLAADw8P3Nzc6Ny5M/fv3wegT58+jBgxgqZNmzJy5Ej++ecf2rRpQ4MGDWjUqBEnTpwAjMXvvLy8qFevHi1atODq1avpjtOcp33tAO3atUMphVIKT09PwsLCTOu++eYbOnfubCqm95Cvr6/ZOkk5yaE/V3JkYlMqrGqOS+xRjud24YDPTAyjb1CwsBPh4eH07NmT9u3bkz9//iRnYVlWahkisz4eeUawfqTW37V7to/1Ix+ZbW1tbXXhwoX1oUOHksT1v//9T8fExGittR48eLBesmTJw+ysV61aZWobERFh+vm1117T/v7+WmutS5UqpaOjo7XWWt+8eTPFcRcvXqxLliypw8PD9f3797Wrq6sOCgrSWv/3TTHxGUFkZKSOiorSWmsdGhqqH/5/btu2TRcoUEBfuHBBx8fHa29vb71z50794MEDXbFiRb13716ttda3b9/WsbGxae4n8Tf1adOm6T59+mittT5+/LguW7asabvkkm97/fp1XblyZdPy+fPnTa9j27ZtukiRIrpOnTq6TZs2+ujRoyn217dvX/3NN9+YPdaYMWP0zJkzTcvh4eGmnz/++GPTut69e+v27dubztqaNWumQ0NDtdZaBwQEmCbEuXHjhjYYDFprrRcsWKBHjBiR4pgnTpzQbm5uZh/J39u0XntyNjY2ukGDBtrLy0v/8ssvKdbHxMToevXq6R07dmittQ4LC9ONGzfWcXFxunfv3knOCOLi4rSTk5PZ42T3M4LwKxf07u8+MH37vzCumj6weXmSNps2bdLFihXTtra2esyYMaa/zayATDoxjWW0nWSVw0oZ6qcrQ22OTqNEc/369fn3339xdHRk/fr1+Pr6curUKVO7mJgY/P39U62nc/ny5STlEo4ePconn3xiKuyX+GqPLl26YGNjw71799i9ezddunQxrXtYLC8sLIyuXbty+fJlYmJiqFixYopjVq9enYMHDz7ydT/qtSeXWgnvh4YMGULjxo1p1KgRAMOHD2fy5MmmaquJ2djYkCdPHu7evZviDC27io+LI2jBm3hfXYkPcEGV5oHvQqq4PYdzsralSpWiWrVqfPvtt9SuXdsa4VpE9ksEViJlqJ+uDLU5Tk5O3Lp1i7i4OGxtbQkLCzOVq36YkMDYBTJkyBDCw8NxcnIC4I8//qB+/fqUKFHC7L7z5s2bpIR0nz59+PXXX3Fzc8PPzy9JaeiHRe0MBgOFChUy+2H+1ltvMWLECDp06GCa7S25kydP0rVrV7PxbN++PckAY1qvPbnUSngDfPrpp1y/fj3J5DzBwcGmbsbw8HDWr1+Pra2t6YvGgwcPrFYFM6Pdu3OTPF9WwVvFEU4hLr4wDdfnO2KbOw9g/P1dtGgRBw4cYPbs2dSqVYudO3dm2hvDnlT2GyOwIilD/WRlqFOjlKJp06am/48lS5aYSk5fuXLFlGT27t2LwWBIUrM/rekrAWrWrGmazP3h6ypVqhSxsbGp9pEXKFCAihUrsnr1asD4IXHo0CHA+P9apkwZU5zmPDwjMPdIfpVJWq89sbRKeC9cuJCNGzeyYsUKcuX670/97NmznDt3jnPnzvHyyy8zZ84cUxKIiIigWLFi5M6dO9X/u+zizNFAHL+qQB4VR1ChtuQfdRy3pl1MSeDMmTO0aNGCAQMGEBISQlRUFJD2F62sShLBM/awDPWECRM4deqUqZxxnTp1aNmyZZK68g8lLkPt6+ubogz1w4HWR5Whrlu3Lp07d35kGeolS5bg7e1NaGjoY5WhdnNzo2XLlqYzGHP7SVyGevr06QwZMoT4+Hhq165N165dzZahBmjUqBFdunRh69atODs7s3HjRgAmT57MV199RZUqVYiIiDDNV7xmzRpq1aqFm5sbw4YNY+XKlaY/0Pv377N582ZeeumlVF9X27Zt2bFjh2l5/PjxeHl50bJlS9NUo+Y8TPJubm64urry22+/ATBu3Di6dOlCo0aNTGclTyu11x4cHEz//v0BY0VQd3d33NzcaNq0aZIS3oMGDeLq1av4+PgkucQ2Ldu2baNdu3bPJP7MKiRgA+c+q02lNa0A4z0BHsNXYmfvABj/7qZPn06tWrUICgpi3rx5bN26NVMViXvWpAy1yLE6derElClTqFq1qrVDyTReeuklJk6caPasLav/nV06e4KoZd2pHG+cqGhvoXY4+46jdIWkr/Xq1atUr16dRo0a8e2335rGzbK6tMpQyxiByLEmTZrE5cuXJREkiImJwdfXN91dd1lF2OmjxC3vRgXDBQCO53bBofNsPGvUN7WJiYlh2bJl9OnThxIlSnDw4EHKly+fLbuBzMk2iSC1QU0hUlO9evVs96H3NPLkyUOvXr3MrstqPQdgnB8gaO5AvK4bx1mO2NUnb7sJ1HRLOoYWFBTE66+/ztGjR3F2dqZVq1ZUqFDBChFbT7YYI7C3tyciIiJL/rIKkdlprYmIiMhSVxJd/vckN8dXNCWBPWX7U/vDbVRJlATu37/Pe++9h7e3Nzdv3sTf359WrVpZK2SryhZnBM7OzoSFhXH9+nVrhyJEtmRvb58l+srDr1zg1G+T8Lm8DICggq1xf3slPrlSfuft2LEjW7ZsYeDAgUyZMoWCBQtmdLiZRrYYLBZCiH9P7Kf8yqYAnM1VgZsew6nftm+SNrdv38bOzg57e3t27NhBfHw8TZs2tUa4GU7mLBZCZGsBS8eYksC+/M2oOOZQiiSwdu1aXF1dTfW4GjdunGOSwKNki64hIUTOdOHUIa7/Nhrve39xBwdiBu2lQcmySdpcv36dt99+mxUrVlC7du007y/JqSQRCCGynHt3bnLl6+ZUif+HssBhew/yNB9FjWRJYNOmTfTo0YPbt2/z6aefMmrUKPLkyWOdoDMxSQRCiCzj+qVznAv4DY/DY6gCXKMIdzotpY7b82bblylThpo1a/Ltt9/i6uqascFmIZIIhBBZwpG/fqb2tr4UA07bVCai2it4dR1F4hkVDAYDCxcu5MCBA6YP/8SlRIR5kgiEEJmaIT6e/V+/gvudLQDsKdULr/4zqJKsjPbp06cZMGAA27dvp2nTpkRFRWXr+kDPkiQCIUSmdT70ILmXv4w7xnuEbg09iY9TySRt4uPjmTFjBqNHjyZ37twsWLCAfv36SaWBx2DRy0eVUm2UUieVUqeVUinme1RKFVRK/a6UOqSUOqaU6mtuP0KInOXfE/sJ+OFTyi1/gVJcJzh/c2I/ukahZEkAjOW3J0yYQMuWLQkJCaF///6SBB6Txc4IlFI2wGygJRAGBCml/LXWIYmavQmEaK1fVEoVA04qpX7QWsdYKi4hROYV+ONUnE7+QOX4s5QHTtlUwdB2Ku7uzZK0e/DgAd9//z39+vUzFYkrV66cJIAnZMmuIU/gtNb6DIBSaiXQEUicCDSQXxnfPUfgBhBnwZiEEJmQNhjYO6s3Xjf8AQh0eomCHt2p7tEClaw8RGBgIP369ePYsWOUL1+eVq1aUb58eWuEnW1YMhGUAS4kWg4DvJK1mQX4A5eA/EBXrbUh+Y6UUgOBgQDlypWzSLBCCOvYt24hDYLexQuI0TbceeMAXqVTfrBHRkYyevRoZsyYQZkyZVi3bl2OLRL3rFkyEZg7R0te2Kg1cBBoBlQGNiuldmqt7yTZSOv5wHww1hqyQKxCiAx2785Nwma2pUHccWK1DcHl+tLgtc9xsjNf5dTX15ctW7YwePBgJk2alGTeavF0LJkIwoDEt/k5Y/zmn1hfYJI2Vr47rZQ6C9QA9lowLiGEFWmDgb0/fYXXsfE8nBT0Wu+d+HMQMFUAACAASURBVFRKecPXrVu3sLOzI2/evIwZM4bRo0fTuHHjjA04B7DkVUNBQFWlVEWlVB6gG8ZuoMTOA80BlFIlgOrAGQvGJISwIm0wcGRKC7yOjSdW2xDoOgbG3aaMmSTg7++fpEhco0aNJAlYiMUSgdY6DhgKbASOAz9qrY8ppQYppQYlNBsPNFRKHQG2AiO11uGWikkIYR1xsTHsXfMV+tMi1InexyF7D9QnV/Dq8m6KtteuXaNbt2507NgRJycnXn75ZStEnLNY9IYyrfV6YH2y5+Ym+vkSIKM9QmRjJwI3UfSPgXhykwfk5p/c1XEZsQ7b3CmLv23YsIEePXpw7949xo8fz8iRI8mdO7cVos5Z5M5iIYTFBM7qi1f4z8RoWwJdP8bz5fdwMTNb2ENly5aldu3azJkzBxcXlwyMNGeTRCCEsIiAHz7FO/xnAG4O2IuXc+UUbQwGA/PmzePgwYPMmzcPV1dXtm/fnsGRCpmhTAjxTN27c5PT4+vjfeor7uDAfp9ZlDCTBEJDQ2nSpAlDhgzh7NmzREdHWyFaAZIIhBDPiCE+nr2/fIPtl1WpEv8P/+Zyxv7Ds9Rv3TNJu7i4OCZPnkydOnU4cuQIixcvZuPGjdjbm79/QFiedA0JIZ5a9P17hMzujmfkDlAQWPMjvLqONNs2IiKCyZMn065dO2bPnk2pUqUyOFqRnCQCIcRTuX0zHP21G/W5R0ie2pR/ay1e+QslafPgwQP8/PwYMGAAJUqU4NChQ5QtWzaVPYqMJl1DQognog0GAuYMwGFGNQpxj8AiHXD56G/yJUsCe/bsoV69egwaNIg///wTQJJAJiOJQAjx2G7fuM7Ryc3xvvYjN1VB9nnOwGvY0iRt7t27x/Dhw3nuueeIjIxkw4YNtGjRwkoRi7RI15AQIt3i4+LY99s3eB4ZR20goGQPvAbOoriZewN8fX3ZunUrQ4cO5YsvviB//vwZH7BIF2Ws95Z1uLu76+DgYGuHIUSOc/tmOAW//u8y0MBiL+P15qIkbW7evIm9vT158+bl77//BuD555/P0DiFeUqpfVprd3PrpGtICPFIe777gAdfewCwP18j7r93PkUS+Pnnn3FxcWHcuHGAMQFIEsga0tU1pJTKC5TTWp+0cDxCiEwkOiqS61Pc8dHGCvLHWq2gfsN2SdpcuXKFoUOH8tNPP1G3bl26detmjVDFU3jkGYFS6kWMk8dsSFiuq5RKXk5aCJHNnD60i/uTa1BWX+J8rjJcG3AQ12RJ4I8//sDFxYW1a9fyxRdfsHfvXurVq2eliMWTSs8ZwTiM8w9vB9BaH1RKVbBYREIIqwtetwD3oPeAtG8OK1++PPXq1WP27NnUqFHDbBuR+aUnEcRprW8b55cXQmRnh/78kehja/GK+M243HgeXs3+6+oxGAzMmTOHQ4cOsWDBAlxcXNi6dau1whXPSHoSwVGl1KuAjVKqKjAM2G3ZsIQQGen0oV2U+dkXNxVDrLYhqFArynSagFuF6qY2J0+epF+/fuzatYvWrVsTHR0t9YGyifRcNfQW4Ao8AJYDt4G3LRmUECJjREdFsmfem1T4+X9EKXsOOvjAx5fxeGc1pROSQGxsLBMnTsTNzY2QkBD8/Pz4448/JAlkI4+8j0Ap1UVrvfpRz2UUuY9AiGcjYOkYvP/5GoCggq2p1ns2BYsUS9Hu2rVr1KhRg+bNm/PNN99QsmTJjA5VPANPex/Bh+l8TgiRBZwNCYJxBU1JIKDqCDze+TFJEoiOjmbOnDkYDAaKFy/O4cOHWb16tSSBbCrVMQKlVFugHVBGKTUz0aoCQJylAxNCPHsBcwbifW0VAKG21Sj/3l942zskafP333/Tr18/QkNDqVatGi1atMDZ2dka4YoMktYZwSUgGIgG9iV6+AOtLR+aEOJZObZrnXHWsGuriNE2HH5hEdU+CcIuURK4e/cuQ4cOpVGjRsTExLBp0yYpEpdDpHpGoLU+BBxSSi3XWsdmYExCiGfk3p2bhHw3BM9b6zFoRaCTLy69plOnYJEUbX19fdm2bRtvv/02EyZMwNHR0QoRC2tIz+WjFZRSEwEXwHSZgNa6ksWiEkI8FW0wEOA3Ep/z8/EE7pCPmDf24FW6fJJ2N27cwN7eHgcHB8aPH49SCh8fH+sELawmPYPFi4FvMY4LNAW+B5amuYUQwioeRN9nz3cfcHF8TXzOzwcgoPLb5B8ThlOyJLBmzRpq1qxpKhLXsGFDSQI5VHoSQV6t9VaMl5r+q7UeBzSzbFhCiMd1ZMdvqIll8Tk/D2d9hYAS3TCMvoF3z89QieYLuHz5Mi+99BJdunShbNmy9OjRw4pRi8wgPV1D0UqpXMAppdRQ4CJQ3LJhCSHSKy42hv2ze+N5az0oCHDuR51uY/F2LJii7bp163jttdeIjo5m8uTJjBgxAltbmZ8qp0vPb8BwwAFjaYnxGLuHelsyKCFE+gQsHYPH6Zl4Ks01inCrgx/e9V9ItX2lSpXw8PBg1qxZVKtWLQMjFZlZmncWK6VsgEla6/czLqS0yZ3FQhjt8RuFz7lvAQioMhyvV8cm6QICiI+PZ9asWRw+fJhFixaZ243IIdK6szjNMwKtdbxSqoFSSumsNqelENnU3ds3yP1VNXyU8aru22//g3dhpxTtQkJC6N+/P3v27KFdu3ZSJE6kKj2DxQeA35RSPZVSLz18WDowIURK/548SNis9tirWM7kqsCd4WcomCwJxMTEMGHCBOrVq0doaCjLli1j7dq1kgREqtIzRlAEiCDplUIa+PlRGyql2gBfAzbAQq31JDNtmgAzgNxAuNY69Q5OIXKwfesX02DvcAACqr2H96ujzba7desW06dPp1OnTsycOZPixeXaDpG2RyYCrXXfJ9lxwvjCbKAlEAYEKaX8tdYhidoUAuYAbbTW55VS8hsrRDJRkXcJm96UBnGniKAg//p8jnfrnknbREWxaNEihgwZQvHixTly5AilS5e2UsQiq7HkdWOewGmt9RkApdRKoCMQkqjNq8DPWuvzAFrraxaMR4gsJ3BmT7xu+FMVY5G4Ym/4U79YqSRtduzYQf/+/Tl16hQ1a9akefPmkgTEY0nPGMGTKgNcSLQclvBcYtWAwkqp7UqpfUqpXuZ2pJQaqJQKVkoFX79+3ULhCpF5BM7syYOxTnjd8AdgT9kBVPskiMKJksCdO3cYMmQIL7zwAnFxcWzZsoXmzZtbK2SRhVnyjMDcJMfJrzyyBRoAzYG8wB6lVIDWOjTJRlrPB+aD8fJRC8QqRKZw51YEx5e9Z0wACgJrfojbi0PxcUhZAM7X15ft27fzzjvvMH78ePLly2eFiEV28MhEoJQqAXwBlNZat1VKuQA+WutHXZQcBpRNtOyMsbR18jbhWutIIFIptQNwA0IRIoc5+rc/tbb0xAs4m6sCJUbswCvZ3cHh4eE4ODjg4ODA559/jlIKb29v6wQsso30dA35ARuBh52OoRjvNn6UIKCqUqqiUioP0A3jXAaJ/QY0UkrZKqUcAC/geHoCFyI72eM3ilpbenJf2xHoOoYKnxzAIVES0FqzcuVKatasydixYwHw8fGRJCCeifR0DTlprX9USn0IoLWOU0rFP2qjhHZDMSYRG+A7rfUxpdSghPVztdbHlVIbgMOAAeMlpkef+NUIkcVE3r3FscVD8bnxOwA3e/+FV6WaSdpcvHiRIUOG4O/vj4eHB716mR1KE+KJpScRRCqlipLQv6+U8gZup2fnWuv1wPpkz81NtjwVmJquaIXIRs6GBGGzpjeehotEUJCbnZZTJVkSWLt2LT169CA2NpZp06YxfPhwbGxsrBSxyK7SkwjexdilU1kptQsoBrxs0aiEyMZOHdxJHv9BVDSEEadzsbdwOzyHr6CombZVqlShYcOGfPPNN1SpUiXDYxU5Q5pF50yNlLIFqmO8EuikNaeulKJzIisL+nUWHgc/BiCg0jCqtBqIU8n/rqmIj49n5syZHDp0CD8/PytFKbKjJy46l7DxIWAVsEpr/c+zDk6InCD80r+cW/EOHne3AhDSZhXe3m2StDl27Bj9+vUjMDCQ9u3bS5E4kWHSc9VQB4zTVP6olApSSr2nlCpn4biEyBaioyI5OLk1TvPr4H53K2GqJDffPIFLoiQQExPDZ599Rr169fjnn39Yvnw5v//+uyQBkWHS1TVkaqxUVWA00ENrbZURK+kaElnBg+j7hOz4Gdddw8iTcJFdcIMpuL/4Roq2165dw8XFhdatWzNjxgyKFSuW0eGKHOCpuoYSdlABeAXoCsQDHzyr4ITITh5E32f/9yPxufQ99YA4chFcfzLuHQaR+C/w/v37LFiwgKFDh5qKxJUqVSq13QphUekZIwjEWCJ6NdDlYRE5IURShyc1p050MD4Jy3tK96ZW13G4FyySpN22bdvo378/Z86coVatWjRv3lySgLCq9JwR9NZan7B4JEJkUYe3rcHh7y+oE2+8lmJPxTep2noQPiWTDqXdvn2bDz74gPnz51O5cmW2bdtGkyZNrBCxEEmlmgiUUq9prZcB7ZRS7ZKv11p/ZdHIhMjkrob9g1rYgjrcAGBv4f/h+vpsfPIXMtve19eXHTt28P777zNu3DgcHBwyMlwhUpXWGcHDUob5zayTCqAiRwteOx/34PcBOGpXF/t2X+Dp9lyKdtevXydfvnw4ODgwceJEbGxs8PDwyOhwhUhTqolAaz0v4cctWutdidcppVL+xguRA+xZPBKff+eaBn73lB2AT79pKdpprVmxYgXDhg2jb9++TJ06VQrEiUwrPWME3wD10/GcENnWtYtnubX4FXzijBXSQ/LURr/wIT7PtU/RNiwsjMGDB7N27Vq8vLzo06dPBkcrxONJa4zAB2gIFFNKjUi0qgDGaqJC5AgBC0fgHbaI4sB+x8aUe20OLiXLmm3r7+/Pa6+9Rnx8PNOnT+ett96SInEi00vrjCAP4JjQJvE4wR2k6JzIAW6FX+Hq3I54xxkvmguuPwn3DoPT3KZatWo8//zzzJo1i0qVKmVEmEI8tUfeWayUKq+1/jeD4nkkubNYZIS42Bguf+FGWX2JUNtq5HvVjzKVXFO2i4tjxowZHD58mO+//94KkQqRPk90Z7FSaobWejgwSymVIltorTs8wxiFyFSufFGbsvoKQQXb4PHOKrNtDh8+TL9+/QgODqZjx45SJE5kWWl1DS1N+DflJRFCZFOBq7+k6rEZOHMHgLpvpvyW/+DBA7744gu++OILihQpwo8//sjLL7+MUiqjwxXimUjr8tF9Cf/+9fA5pVRhoKzW+nAGxCZEhjm8bQ0V/hqGF5EAHM/tQunB/hTMY5ei7Z07d5gzZw7du3dn+vTpFC1qbkoZIbKO9NQa2o6xFLUtcBC4rpT6S2s9Is0Nhcjk4uPi2Pf7HKocmkqdhDOAs7nKw8uLqOmS9KavyMhI5s+fz7BhwyhWrBhHjx6lRIkS1ghbiGcuPfcRFNRa31FK9QcWa63HKqXkjEBkaRFXw3CcUxdPFcsFVZqTpXyp2PZtKpZNOR3k1q1bGTBgAGfPnsXNzY1mzZpJEhDZSnomprFVSpXCWIZ6rYXjEcKitMHAnvnDKPqtK3Yqln35m1Lqo0P4DPyGksmSwK1bt+jfvz8tWrTA1taWv/76i2bNmlkpciEsJz1nBJ8BG4FdWusgpVQl4JRlwxLi2Yu+f4/YyVXwUVHEaBv2VRqET+8vUm3fqVMndu7cyciRIxk7dix58+bNwGiFyDiPNUNZZiD3EYjHpQ0GAha+jc8l4xVAYaokpT8JIZeZO36vXr2Ko6Mj+fLlIzAwEFtbWxo0aJDRIQvxzKV1H8Eju4aUUs5KqV+UUteUUleVUj8ppZyffZhCWEbgvMGmJLA/XyOcx55MkQS01ixduhQXFxfGjh0LgJeXlyQBkSOkp2toMbAc6JKw/FrCcy0tFZQQz0Lo/r/IvfYtvA3/Eq8VsSPDqO/gmKLd+fPnGTRoEH/88Qc+Pj7069fPCtEKYT3pGSwuprVerLWOS3j4ATK7tsi0Dm5dCeMKUs2/A2XiL3LAoSH3hp/G3kwS+O2333B1dWXHjh3MnDmTnTt3UrNmTStELYT1pOeMIFwp9RqwImG5OxBhuZCEeDLaYCBg0Tv4XPQDYG/h9lTr8RX1nEqmbKs1Silq1KhBkyZN+Oabb6hQoUKGxitEZpGeonPlgFlgmpN7F/C2tQrRyWCxMMc4IDwcn0tLADjWagWuDVPMsEpcXBxffvklR44cYdmyZRkdphBW80RF5x7SWp/HeGexEJlOXGwMR7evIf7QKnzubQfg1tCTuJo5Czh06BCvv/46+/fvp1OnTlIkTogE6blqqJJS6nel1PWEK4d+S7iXQAirCt3/F7afF6PursE0uLedo3Z1uTX0JIWSJYHo6Gg++eQT3N3duXjxImvWrOHnn3+WJCBEgvSMESwHZgOdEpa7YRwv8LJUUEKkxRAfT+CSD6n/73eg4IBDQyr1/55aRcxfw3D37l3mzZtHjx49+OqrryhSpEgGRyxE5paeq4aU1nppoquGlgHpugtNKdVGKXVSKXVaKTUqjXYeSql4pZTMfCbSpA0Gjk5tjc/5edipWI4086PeB39QMFkSuHfvHtOmTSM+Pp5ixYoREhKCn5+fJAEhzEjPGcG2hA/xlRgTQFdgnVKqCIDW+oa5jZRSNhjPJFoCYUCQUspfax1ipt1kjGUshEhVVORdoqfWpA53CVMlKfXxMWrbpvwV3rRpEwMHDuT8+fM0aNCApk2bUqyYXPEsRGrSc0bQFXgD2AZsBwYDrwP7gLQu3/EETmutz2itYzAmko5m2r0F/ARcS3/YIqcJ+nU21770oTB3CSjRjdKfhGCTLAncuHGDvn370rp1a+zt7dm5cydNmza1UsRCZB3puWqo4hPuuwxwIdFyGMnGFZRSZTCOPTQDkhaAT9puIDAQoFy5ck8YjshqtMHA3jVfUj5kLh6EAxBQaRjevcabbd+pUyd27drFRx99xOjRo2UwWIh0Sk/X0JMyN29f8rGFGcBIrXV8WtP8aa3nA/PBeB/BM4tQZFqRd2+R78vypm8Oh/J64tRlJt6Vkt71e+XKFfLnz0++fPmYOnUqefLkoW7duhkfsBBZmCUTQRhQNtGyM3ApWRt3YGVCEnAC2iml4rTWv1owLpGJGeLj2f91V9zvbAbgGkWwG7YXt2SDwVprlixZwogRI+jbty9ffvklnp6e1ghZiCzPkokgCKiqlKoIXMR42emriRsk7nZSSvkBayUJ5Ez3793m+I41lN07AXduEKNtOVh3HJ6d3krR9ty5c7zxxhts2rSJ559/noEDB1ohYiGyj/TMWayAHkAlrfVnCSUnSmqt96a1ndY6Tik1FOPVQDbAd1rrY0qpQQnr5z59+CI7OLBxCfX2DKMBEE4hAqq9j3uXD/DMnSdF219++YWePXuilGLWrFkMHjyYXLnSc82DECI16TkjmAMYMA7ofgbcxXiVT6qDuw9prdcD65M9ZzYBaK37pCMWkc3snfkanjd+ByDQdTS12w7A27FginYPi8S5urrSokULvv76a8qXL5/R4QqRLaUnEXhpresrpQ4AaK1vKqVSflUT4jGFTvDAMy4UgLNdNuHlmvJm9djYWKZOncrRo0dZvnw51apV49dfpfdQiGcpPefUsQk3fWkApVQxjGcIQjyR+Lg4ro2rSLW4UE7ZVCHmw6tUNJME9u/fj6enJx9//DHx8fE8ePDACtEKkf2lJxHMBH4BiiulPgf+BlKf8VuIVJw5GkjI588RP74ExTHekF7mnT/JY5f0ev+oqCg+/PBDPD09uXLlCr/88gurVq3Czs7OGmELke2l54ayH5RS+4DmGO8N8NVaH7d4ZCLbuBV+hROrx+J9dSUA52zKEd5gGPVa98XBTImIyMhIFi1aRO/evZk2bRqFCxfO6JCFyFHSc9VQOeA+8Hvi5xLmKRAiTYb4eK7NfRHvuFCO5XHD8Nxwar/wEhWStbt79y7ffvst7777Lk5OToSEhODk5GSNkIXIcdIzWLwO4/iAAuyBisBJwNWCcYls4OyxQGzX9KGavkRwgRa4j/jJbLsNGzbwxhtvcOHCBTw9PWnSpIkkASEyUHq6hmonXlZK1cdYhE4Is25ev8z573rjFhUIQECFN/HqNSFFu4iICEaMGMH3339PzZo12bVrFz4+PinaCSEs67HvLNZa71dKPfIeApEznTkaiPPqtripeK5RhPDWc/D2aWu27UsvvcTu3bsZPXo0H3/8sQwGC2El6RkjGJFoMRdQH7husYhElhW6fzvV/DuCgkCnl/AaupjiydpcvnyZ/Pnz4+joyLRp08iTJw9ubm5WiVcIYZSey0fzJ3rYYRwzMDevgMjB9m9cakwCQHCDKXgNXZxkvdaa7777jpo1azJmzBgAPDw8JAkIkQmkeUaQcCOZo9b6/QyKR2Qx2mDgyI5fcN39DrdVPq52WIF7/ReStDlz5gxvvPEGW7ZsoXHjxgwaNMhK0QohzEk1ESilbBMKx9XPyIBE1hDzIJr9Sz+iSthP1OEW4aoQ91/9jWrVks4F8PPPP9OzZ09sbGz49ttvGThwoBSJEyKTSeuMYC/G8YCDSil/YDUQ+XCl1vpnC8cmMqlg/2+pvW803irWuJy/OTUHfodT/kKmNg+LxNWuXZs2bdowY8YMypYtm9ouhRBWlJ6rhooAERirjz68n0ADkghymNsRV4n+xgd3IogjFwFVR1Cv8/u42zuY2sTExDBlyhSOHTvG8uXLqVq1Kj/9ZP7+ASFE5pBWIiiecMXQUf5LAA/JdJE5zNWwfyixsD4FgX9sKlF82Fa8CxZJ0iY4OJh+/fpx+PBhunXrRkxMjFwSKkQWkFZnrQ3gmPDIn+jnhw+RA8TFxnBkxy+UWGgcKtpTuheVRx8gf6IkEBUVxQcffICXlxfh4eH89ttvrFixQpKAEFlEWmcEl7XWn2VYJCLTObZ7PUU3vUVtwgHYl78ZPgO/SdEuMjISPz8/+vXrx5QpUyhUqFCKNkKIzCutRKDSWCeyuZvXL+O6qTsAgTU/olqzXjQoVsq0/s6dO8yZM4f3338fJycnjh8/TtGiRa0VrhDiKaTVNdQ8w6IQmYY2GDi4ZQWFZ9cAYI/z63h1HUnhRElg3bp1uLq68vHHH7Nz504ASQJCZGGpJgKt9Y2MDERY36E/f0R9Vpi6fw/iOoU54DMTn/7TTeuvX79Ojx49+N///kfBggXZvXs3TZo0sV7AQohn4rGLzonsRxsMBCx6B5+LfoBxLKDWmz9QL9FloQCdO3cmICCAcePG8eGHH5Inj0xdLUR2IIkghztzNJA8P/XCR1/hTK4K2LyymAY1/ruZ/OLFixQsWBBHR0emT5+OnZ0dtWrVsmLEQohnTe71z8H2LB5JpTWtcNZX2FuoHRU+3k/5hCSgtWbBggW4uLiYisQ1aNBAkoAQ2ZCcEeRAhvh4Tk/0xicuFICjzb/Hs9F/BWX/+ecfBgwYwLZt22jatClvvvmmtUIVQmQASQQ5zMUzxymypDHVVAzxWnHt9UBqla9uWr9mzRp69epF7ty5mT9/Pv3790cpuZJYiOxMuoZykMBZfSnzvTd5VQyBRTqQa+wNSiUkAa2NVUPc3Nxo3749x44dY8CAAZIEhMgB5IwgB7j870murBqO1/3dAOzz+BLPtq+jcuUiJiaGiRMnEhISwsqVK6latSqrV6+2csRCiIwkZwTZXPT9e5Ra7Em9+7s5bO/BxV4BNGjfH5UrF3v37qVBgwaMGzcOW1tbYmJirB2uEMIKJBFkU7ExDzi8/Sfsp5QB4GBeb+qM2kKZSjW5f/8+7733Hj4+Pty8eZPff/+dH374QYrECZFDSddQNhR+5QK553pRJ2EeoaCCrfF450fT+qioKJYtW8bAgQOZPHkyBQoUsFaoQohMwKJnBEqpNkqpk0qp00qpUWbW91BKHU547FZKyUzmT0EbDASumozT3FoUJJJ9jk24NuAgHu/8yO3bt/n888+Ji4ujaNGiHD9+nG+//VaSgBDCcmcECRPfzwZaAmFAkFLKX2sdkqjZWeAFrfVNpVRbYD7gZamYsrMTwVvJtWEUXgn3BgSU6I734LkA/P777wwaNIgrV67w3HPP0aRJEwoXLmzNcIUQmYglzwg8gdNa6zNa6xhgJdAxcQOt9W6t9c2ExQDA2YLxZFt75r1FjbUvUS0ulKBCbbn9Vijeg+dy/fp1unfvTocOHShatCiBgYFSJE4IkYIlxwjKABcSLYeR9rf9fsAf5lYopQYCAwHKlSv3rOLL8qLv3+PqNC98DGFEansudlyFR/0mpvUPi8R99tlnjBw5UorECSHMsmQiMHcnktm5jpVSTTEmgufNrddaz8fYbYS7u7vMlwwc27UO182vUh64SQHyfXSKanb2hIWFUahQIRwdHZkxYwZ2dna4urpaO1whRCZmya6hMKBsomVn4FLyRkqpOsBCoKPWOsKC8WQbF04fwXXzqwDsKd2bgqPPYZs7D/PmzcPFxYXRo0cDUL9+fUkCQohHsmQiCAKqKqUqKqXyAN0A/8QNlFLlgJ+BnlrrUAvGkm2E7v+LssuMJ06BNUbhM3Am/5w5Q7NmzRg0aBCenp689dZbVo5SCJGVWKxrSGsdp5QaCmwEbIDvtNbHlFKDEtbPBcYARYE5CTVt4rTW7paKKas7czSQav4dAAiuPwmvDoNZvXo1vXr1ws7OjkWLFtG3b1+pDySEeCwWvaFMa70eWJ/submJfu4P9LdkDNlFdFQkedf0ACDQdTSeLw4CoF69enTs2JGvvvqK0qVLWzNEIUQWJSUmsgBtMHBpakNKcZ3dZfqy7sgdXnnlFbTWVKlShZUrV0oSEEI8MUkEWcC+GV2oZDjHaVWOATM2M378QrXyQwAADTRJREFUePLmzStF4oQQz4TUGsrE4mJjOPR1F9zvbcegFa5fnKREyZKsX7+etm3bWjs8IUQ2IYkgk4q8e4sbX/nQQBuvuPXxL8WAgc2ZOHEi+fPnt3J0QojsRBJBJnTqwA6q/vYi+YCzucrjPCqIjcOjKFSokLVDE0JkQzJGkMn8v727j7aqrvM4/v5cuDw/P6SG8ZADmA9IQMBgEOjIKE6ZpVFSpKmkiWPYuGAcM7RZBdMURkgsH6hxKnEwFdQQHZGB1Esgc+VhBBei6U0NaBBBQR7ud/7Y2zpzuRcOeB4493xea51199n7d87+fte563zP3mef7+/3G1bTe8GnAXhkx0d5dcRMKps1dxEws7zxEcExYu97e6i6+1uM2PILAO7adgYfv2wGAwcOLHJkZtbYuRAcA55/aj4dlk9lRG0NALP4Ml+fcRuVlZVFjszMyoELQRHt2/se1T/6LJ/Yk0wq/2T7z3PcOZOYeNrpRY7MzMqJC0GRHNi/n6rvnctwVvNS7QlUjP03zj7Vc/KYWeH5y+IiWL7oP2jyz50ZzmoAar+8gF4uAmZWJC4EBVR74AAPTr+c4SuuBGBJxTD2TH6d3n36FjkyMytnPjVUIGuXL+Ck/7ySC/UeACuG3clZo79Q5KjMzFwI8m7Pnj3M/u51XF85DwTPfng8p1x0M0M6dS12aGZmgAtBXi26/2ccv/qHXN/sNWpqO7P93Nv562HuEWRmxxYXgjzY8sc3WDTjG4xrvpSmzWp5tM3FnHPtLE5s3qLYoZmZHcSFIIfe2/Mur/zrSPru38hXW0BNbRd2X3wf55/mSdfM7NjlQpAjL65fTZ/5o+gLrG92Ott7jmHQhdfRomXrYodmZnZILgQ58Ku5t3PJqzcCUF1xKv1v/G2RIzIzy54LwQfw+h/+wCM/vIIJ7ZIWEctbns3wyQ8UOSozsyPjQnCUag8coGrGWCa0WwvA6mGzGT56XJGjMjM7ci4ER2jF0keprJrJgD1VfK5Nsm7npJcZ0L5TcQMzMztKLgRZqq2tZdbMGfz9W7cCsI0OvNRpBKd/bRZt27QvcnRmZkfPhSALGzZsYNat1zGrTxUAS7uOY+Q1s+lS5LjMzHLBheAw5s2bxxP3TOfuwZsB+F3H8xl5zewiR2VmljsuBA2ora2loqKCrns3/7kIrBowjcGfubrIkZmZ5ZYLQR27d+/mlltuYePGjUwZO5SzN08H4KXPLWJQv2FFjs7MLPdcCDIsX76cK664gre2vcmGia3puGEJkBwJuAiYWWPlQgDs3LmTKVMms++Vp3n8C/vo0QTgHd6mFZU3vMig1m2LHaKZWd6UfSHYv28v1Qt+wrc7P8LxXXcAsLb5x6kYcQOnnnl+kaMzM8u/si0E27Zu5Rczv82XmjzOcP7EmxVdWPGxf+Tkcy7ndE8aY2ZlJK+FQNK5wI+BJsBdETGtznal28cA7wKXRsTqfMYUEfxqzr8wsGYu36x8E4CqD41lyFVzOL7CUzibWfnJWyGQ1AS4HTgHqAFWSloYEf+TMew8oHd6GwL8NP2bF8/9djEtH7uecU1reL1JRx7vejn9Pn01Q7v3ztcuzcyOefk8IhgMbIqIzQCS5gEXAJmF4ALgnogIoEpSB0knRMQbuQ5m7bIHOfXJK6loUsvT0Y/TJj3E6I6dc70bM7OSk89C0A14LeN+DQd/2q9vTDfg/xUCSROACQDdu3c/qmDaHdeLFypPYd+oqZx55llH9RxmZo1RPguB6lkXRzGGiLgDuANg0KBBB23PRo++/elx07KjeaiZWaOWz29Ha4CPZNw/EXj9KMaYmVke5bMQrAR6S+olqRnwRWBhnTELgfFKDAV25OP7ATMza1jeTg1FxH5JE4HFJJePzo2I9ZKuSrfPAX5DcunoJpLLRy/LVzxmZla/vP6OICJ+Q/Jmn7luTsZyANfkMwYzMzs0/4LKzKzMuRCYmZU5FwIzszLnQmBmVuaUfF9bOiRtBX5/lA/vAmzLYTilwDmXB+dcHj5Izj0iot7WyiVXCD4ISasiYlCx4ygk51wenHN5yFfOPjVkZlbmXAjMzMpcuRWCO4odQBE45/LgnMtDXnIuq+8IzMzsYOV2RGBmZnW4EJiZlblGWQgknStpo6RNkqbUs12SZqbb10gaUIw4cymLnMelua6R9IykM4oRZy4dLueMcZ+QdEDSRYWMLx+yyVnSSEnVktZL+q9Cx5hrWfxvt5f0sKTn05xLuouxpLmStkha18D23L9/RUSjupG0vH4J+CjQDHgeOKXOmDHAIpIZ0oYCK4oddwFyHgZ0TJfPK4ecM8YtIemCe1Gx4y7A69yBZF7w7un9DxU77gLkfCMwPV3uCvwv0KzYsX+AnEcAA4B1DWzP+ftXYzwiGAxsiojNEbEXmAdcUGfMBcA9kagCOkg6odCB5tBhc46IZyJie3q3imQ2uFKWzesMcC3wa2BLIYPLk2xyvgR4ICJeBYiIUs87m5wDaCtJQBuSQrC/sGHmTkQsI8mhITl//2qMhaAb8FrG/Zp03ZGOKSVHms/lJJ8oStlhc5bUDbgQmEPjkM3r3AfoKGmppOckjS9YdPmRTc6zgI+RTHO7FrguImoLE15R5Pz9K68T0xSJ6llX9xrZbMaUkqzzkTSKpBB8Mq8R5V82Od8GTI6IA8mHxZKXTc5NgYHA2UBL4FlJVRHxYr6Dy5Nscv5boBo4CzgJeELS8oh4O9/BFUnO378aYyGoAT6Scf9Ekk8KRzqmlGSVj6R+wF3AeRHxpwLFli/Z5DwImJcWgS7AGEn7I+KhwoSYc9n+b2+LiHeAdyQtA84ASrUQZJPzZcC0SE6gb5L0MnAy8LvChFhwOX//aoynhlYCvSX1ktQM+CKwsM6YhcD49Nv3ocCOiHij0IHm0GFzltQdeAD4Sgl/Osx02JwjoldE9IyInsD9wDdKuAhAdv/bC4DhkppKagUMAV4ocJy5lE3Or5IcASHpOKAvsLmgURZWzt+/Gt0RQUTslzQRWExyxcHciFgv6ap0+xySK0jGAJuAd0k+UZSsLHO+GegMzE4/Ie+PEu7cmGXOjUo2OUfEC5IeA9YAtcBdEVHvZYilIMvX+bvAzyWtJTltMjkiSrY9taR7gZFAF0k1wHeASsjf+5dbTJiZlbnGeGrIzMyOgAuBmVmZcyEwMytzLgRmZmXOhcDMrMy5ENgxK+0YWp1x63mIsbsKF1nDJH1Y0v3pcn9JYzK2feZQXVLzEEtPSZcUan9Wunz5qB2zJO2KiDa5Hlsoki4FBkXExDzuo2lE1NtgTdJI4B8i4u/ytX9rHHxEYCVDUhtJT0paLWmtpIO6jUo6QdKy9AhinaTh6frRkp5NHztf0kFFI23UdpuS+RrWSRqcru8k6aG093tV2qoDSZ/KOFr5b0lt00/h69Jfwd4KjE23j5V0qaRZSvrnvyKpIn2eVpJek1Qp6SRJj6UN45ZLOrmeOKdKukPS48A96T6Xp7mtljQsHTqN5FfG1ZImSWoi6QeSVqa5fD1HL42VumL33vbNt4ZuwAGSZmLVwIMkv4Rvl27rQvLLyvePanelf78F/FO63ARom45dBrRO108Gbq5nf0uBO9PlEaT94IGfAN9Jl88CqtPlh4Ez0+U2aXw9Mx53KTAr4/n/fJ+kFcSodHksyS+AAZ4EeqfLQ4Al9cQ5FXgOaJnebwW0SJd7A6vS5ZHAIxmPmwDclC43B1YBvYr9OvtW/FujazFhjcruiOj//h1JlcD3JI0gaZ/QDTgOeDPjMSuBuenYhyKiWtKngFOAp9P2Gs2AZxvY572Q9ISX1E5SB5JOrZ9P1y+R1FlSe+Bp4EeSfkkyB0CNsu9yeh9JAXiKpH/O7PQoZRgwP+N5mjfw+IURsTtdrgRmSepPUjz7NPCY0UA//WWmtvYkhePlbIO2xsmFwErJOJIZqAZGxD5JrwAtMgekb+AjgPOBf5f0A2A78EREfCmLfdT90ixooO1vREyT9ChJ35cqSX8D7Mkyl4XA9yV1ImkbvQRoDbyVWfwO4Z2M5UnAH0m6jFYcIgYB10bE4ixjtDLh7wislLQHtqRFYBTQo+4AST3SMXcCd5NM+VcFnCnpr9IxrSQ19Kl5bDrmkyRdHXeQnFYal64fSdLm+W1JJ0XE2oiYTnKape75/J0kp6YOEhG7SNok/5jk9M2BSPrnvyzp4nRfUnZzS7cH3ohkMpavkJwSq2//i4Gr06MlJPWR1DqL57dGzkcEVkp+CTwsaRXJ9wYb6hkzErhB0j5gFzA+IramV/DcK+n9Uy03UX+P/u2SngHaAV9L100FfiZpDUm3x6+m67+ZFqQDJPMELwIypwx8CpgiqRr4fj37ug+Yn8b8vnHATyXdRHLKZx7JPL2HMhv4dVpAnuIvRwtrgP2Sngd+TlJ0egKrlZx72gp89jDPbWXAl4+apSQtJbncclWxYzErJJ8aMjMrcz4iMDMrcz4iMDMrcy4EZmZlzoXAzKzMuRCYmZU5FwIzszL3f4FR/XDrHCBUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras bilancato 597 (area = {:.3f})'.format(auc_keras_sb_597))\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras bilancato 1057 (area = {:.3f})'.format(auc_keras_sb_1057))\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef046168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
